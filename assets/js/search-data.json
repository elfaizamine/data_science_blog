{
  
    
        "post0": {
            "title": "Introducing fastpages",
            "content": "Training Feed Forward Neural Network . BackPropagation and Gradient Descent vs Genetic Algorithm . The goal of this project is to asses the use of genetic algorithm in training neural network and updating weights vs the use of backpropagation algorithm. . import matplotlib.pyplot as plt from load_data import load_dataset import backprop_functions as bp from PIL import Image import seaborn as sns import numpy as np import h5py import scipy import random %matplotlib inline . from deap import base from deap import creator from deap import tools import elitism from random import randint . train_set_x_orig, train_set_y, test_set_x_orig, test_set_y, classes = load_dataset() . index = 2 plt.imshow(train_set_x_orig[index]) print (&quot;y = &quot; + str(train_set_y[:, index]) + &quot;, it&#39;s a &#39;&quot; + classes[np.squeeze(train_set_y[:, index])].decode(&quot;utf-8&quot;) + &quot;&#39; picture.&quot;) . y = [1], it&#39;s a &#39;cat&#39; picture. . m_train = train_set_x_orig.shape[0] m_test = test_set_x_orig.shape[0] num_px = train_set_x_orig.shape[1] print (&quot;Number of training examples: m_train = &quot; + str(m_train)) print (&quot;Number of testing examples: m_test = &quot; + str(m_test)) print (&quot;Height/Width of each image: num_px = &quot; + str(num_px)) print (&quot;Each image is of size: (&quot; + str(num_px) + &quot;, &quot; + str(num_px) + &quot;, 3)&quot;) print (&quot;train_set_x shape: &quot; + str(train_set_x_orig.shape)) print (&quot;train_set_y shape: &quot; + str(train_set_y.shape)) print (&quot;test_set_x shape: &quot; + str(test_set_x_orig.shape)) print (&quot;test_set_y shape: &quot; + str(test_set_y.shape)) . Number of training examples: m_train = 209 Number of testing examples: m_test = 50 Height/Width of each image: num_px = 64 Each image is of size: (64, 64, 3) train_set_x shape: (209, 64, 64, 3) train_set_y shape: (1, 209) test_set_x shape: (50, 64, 64, 3) test_set_y shape: (1, 50) . train_set_x_flatten = train_set_x_orig.reshape(train_set_x_orig.shape[0], -1).T test_set_x_flatten = test_set_x_orig.reshape(test_set_x_orig.shape[0], -1).T # Standarize the inputs train_set_x = train_set_x_flatten/255. test_set_x = test_set_x_flatten/255. print (&quot;train_set_x_flatten shape: &quot; + str(train_set_x_flatten.shape)) print (&quot;train_set_y shape: &quot; + str(train_set_y.shape)) print (&quot;test_set_x_flatten shape: &quot; + str(test_set_x_flatten.shape)) print (&quot;test_set_y shape: &quot; + str(test_set_y.shape)) print (&quot;sanity check after reshaping: &quot; + str(train_set_x_flatten[0:5,0])) . train_set_x_flatten shape: (12288, 209) train_set_y shape: (1, 209) test_set_x_flatten shape: (12288, 50) test_set_y shape: (1, 50) sanity check after reshaping: [17 31 56 22 33] . BackPropagation . This BackPropagation part is taken from DeepLearning.ai Course . def model(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.5, print_cost = False): &quot;&quot;&quot; Builds the logistic regression model by calling the function you&#39;ve implemented previously Arguments: X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train) Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train) X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test) Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test) num_iterations -- hyperparameter representing the number of iterations to optimize the parameters learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize() print_cost -- Set to true to print the cost every 100 iterations Returns: d -- dictionary containing information about the model. &quot;&quot;&quot; # initialize parameters with zeros w, b = bp.initialize_with_zeros(X_train.shape[0]) # Gradient descent parameters, grads, costs = bp.optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost) # Retrieve parameters w and b from dictionary &quot;parameters&quot; w = parameters[&quot;w&quot;] b = parameters[&quot;b&quot;] # Predict test/train set examples Y_prediction_test = bp.predict(w, b, X_test) Y_prediction_train = bp.predict(w, b, X_train) ### END CODE HERE ### # Print train/test Errors print(&quot;train accuracy: {} %&quot;.format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100)) print(&quot;test accuracy: {} %&quot;.format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100)) d = {&quot;costs&quot;: costs, &quot;Y_prediction_test&quot;: Y_prediction_test, &quot;Y_prediction_train&quot; : Y_prediction_train, &quot;w&quot; : w, &quot;b&quot; : b, &quot;learning_rate&quot; : learning_rate, &quot;num_iterations&quot;: num_iterations} return d . result = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = 2000, learning_rate = 0.005, print_cost = True) . Cost after iteration 0: 0.693147 Cost after iteration 100: 0.584508 Cost after iteration 200: 0.466949 Cost after iteration 300: 0.376007 Cost after iteration 400: 0.331463 Cost after iteration 500: 0.303273 Cost after iteration 600: 0.279880 Cost after iteration 700: 0.260042 Cost after iteration 800: 0.242941 Cost after iteration 900: 0.228004 Cost after iteration 1000: 0.214820 Cost after iteration 1100: 0.203078 Cost after iteration 1200: 0.192544 Cost after iteration 1300: 0.183033 Cost after iteration 1400: 0.174399 Cost after iteration 1500: 0.166521 Cost after iteration 1600: 0.159305 Cost after iteration 1700: 0.152667 Cost after iteration 1800: 0.146542 Cost after iteration 1900: 0.140872 train accuracy: 99.04306220095694 % test accuracy: 70.0 % . Genetic Algorithm . In this part I am going to train a one layer neural network using genetic algorithm. In other words, finding the optimal set of values &quot;Weight Vector&quot; that will minimize the cost function simply through iteration and Fitness values. . The chromosome representation of the GA will the the weight vector itself of the Neural Network. | We will start with a population of chromosome (potential solutions) of POPULATION_SIZE. | Apply a selection process &quot;Tournament selection&quot; to choose the best parents of the next generation. | Apply a crossover process &quot;SimulatedBinaryBounded&quot; within boundaries BOUND_LOW, BOUND_UP | Apply a mutation process &quot;mutPolynomialBounded&quot; within boundaries BOUND_LOW, BOUND_UP | Use elitism to pass the x best solution fom generation G to G+1 with no modification. | Repeat the process until a stopping condition is reached : here is MAX_GENERATIONS. | . def randomFloat(low , up): return [random.uniform(l, u) for l, u in zip([low] * (DIMENSIONS+1), [up] * (DIMENSIONS+1))] # Fitness function : score that will let the GA knows wich solution to choose to be the parents of the next generation def cross_entropy_loss(individual, X_train=train_set_x, Y_train=train_set_y) : eps = 1e-5 m = X_train.shape[1] w = np.array(individual[:-1]) b = individual[-1] A = bp.sigmoid(np.dot(w.T,X_train)+b) cost = round(-(1/m) * np.sum(Y_train*np.log(A+eps)+(1-Y_train)*np.log(1-A+eps)),3) , return cost . DIMENSIONS = train_set_x.shape[0] # Length of chromosome thar reprensent the weights BOUND_LOW, BOUND_UP = -0.1, 0.1 # Boundaries of weight values POPULATION_SIZE = 100 # Population size P_CROSSOVER = 0.7 # probability for crossover P_MUTATION = 0.3 # probability for mutating an individual MAX_GENERATIONS = 100 # max generations to ierate HALL_OF_FAME_SIZE = 20 # Number of individuals to pass from G to G+1 without crossover nor mutation CROWDING_FACTOR = 10.0 # crowding factor for crossover and mutation . random.seed(42) # Definition of algorithms to compute the GA pipeline using DEAP toolbox = base.Toolbox() creator.create(&quot;FitnessMin&quot;, base.Fitness, weights=(-1.0,)) creator.create(&quot;Individual&quot;, list, fitness=creator.FitnessMin) toolbox.register(&quot;attrFloat&quot;, randomFloat, BOUND_LOW, BOUND_UP) toolbox.register(&quot;individualCreator&quot;, tools.initIterate, creator.Individual, toolbox.attrFloat) toolbox.register(&quot;populationCreator&quot;, tools.initRepeat, list, toolbox.individualCreator) toolbox.register(&quot;evaluate&quot;, cross_entropy_loss) toolbox.register(&quot;select&quot;, tools.selTournament,tournsize = 4) toolbox.register(&quot;mate&quot;, tools.cxSimulatedBinaryBounded, low=BOUND_LOW, up=BOUND_UP, eta=CROWDING_FACTOR) toolbox.register(&quot;mutate&quot;, tools.mutPolynomialBounded, low=BOUND_LOW, up=BOUND_UP, eta=CROWDING_FACTOR, indpb=10.0/DIMENSIONS) . def model(toolbox, X_train, Y_train, X_test, Y_test): &quot;&quot;&quot; Builds the logistic regression model by calling the function you&#39;ve implemented previously but training with GA algorithm Arguments: X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train) Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train) X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test) Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test) toolbox -- GA class that saves optimisation parameters and algorithms to be used later Returns: d -- dictionary containing information about the model. &quot;&quot;&quot; population = toolbox.populationCreator(n=POPULATION_SIZE) # prepare the statistics object: stats = tools.Statistics(lambda ind: ind.fitness.values) stats.register(&quot;min&quot;, np.min) stats.register(&quot;avg&quot;, np.mean) # define the hall-of-fame object: hof = tools.HallOfFame(HALL_OF_FAME_SIZE) # perform the Genetic Algorithm flow with elitism: population, logbook = elitism.eaSimpleWithElitism(population, toolbox, cxpb=P_CROSSOVER, mutpb=P_MUTATION, ngen=MAX_GENERATIONS, stats=stats, halloffame=hof, verbose=True) # Infos on best solution found: best = hof.items[0] # extract statistics: minFitnessValues, meanFitnessValues = logbook.select(&quot;min&quot;, &quot;avg&quot;) # plot statistics: sns.set_style(&quot;whitegrid&quot;) plt.plot(minFitnessValues, color=&#39;red&#39;) plt.plot(meanFitnessValues, color=&#39;green&#39;) plt.xlabel(&#39;Generation&#39;) plt.ylabel(&#39;Min / Average Fitness&#39;) plt.title(&#39;Min and Average fitness over Generations&#39;) plt.show() # Retrieve parameters w and b from dictionary &quot;parameters&quot; w = np.array(best[:-1]) b = best[-1] # Predict test/train Y_prediction_test = bp.predict(w, b, X_test) Y_prediction_train = bp.predict(w, b, X_train) print(&quot;train accuracy: {} %&quot;.format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100)) print(&quot;test accuracy: {} %&quot;.format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100)) d = {&quot;costs&quot;: best.fitness.values[0], &quot;Y_prediction_test&quot;: Y_prediction_test, &quot;Y_prediction_train&quot; : Y_prediction_train, &quot;w&quot; : w, &quot;b&quot; : b} return d . Result = model(toolbox, train_set_x, train_set_y, test_set_x, test_set_y) . gen nevals min avg 0 100 2.241 3.94754 1 58 2.211 2.99987 2 65 1.789 2.83352 3 55 1.789 2.60072 4 55 1.602 2.42983 5 62 1.602 2.33102 6 52 1.602 2.37952 7 57 1.539 2.04481 8 51 1.539 2.04603 9 58 1.539 2.12531 10 61 1.403 1.9389 11 60 1.403 1.95289 12 51 1.403 1.83036 13 61 1.384 1.86647 14 58 1.376 1.56289 15 55 1.349 1.4422 16 54 1.344 1.39843 17 54 1.344 1.37828 18 54 1.336 1.36181 19 58 1.329 1.3518 20 47 1.326 1.34517 21 57 1.326 1.34066 22 56 1.326 1.33443 23 58 1.316 1.3306 24 56 1.313 1.32599 25 50 1.311 1.32098 26 58 1.308 1.31675 27 57 1.307 1.31374 28 54 1.303 1.31182 29 53 1.297 1.30938 30 50 1.296 1.30667 31 52 1.294 1.30319 32 54 1.291 1.29944 33 56 1.284 1.29604 34 56 1.279 1.29231 35 51 1.279 1.28843 36 56 1.273 1.28546 37 61 1.273 1.28224 38 52 1.27 1.27894 39 54 1.258 1.27544 40 57 1.258 1.27193 41 58 1.256 1.26717 42 56 1.253 1.26292 43 59 1.252 1.25916 44 58 1.247 1.25704 45 56 1.247 1.25423 46 60 1.241 1.25164 47 55 1.241 1.24831 48 56 1.236 1.24513 49 56 1.235 1.24251 50 56 1.231 1.2396 51 57 1.227 1.23698 52 51 1.225 1.23418 53 54 1.225 1.23115 54 61 1.221 1.22855 55 54 1.22 1.2256 56 55 1.217 1.22441 57 55 1.213 1.2211 58 54 1.213 1.21911 59 54 1.209 1.21672 60 58 1.207 1.21456 61 55 1.205 1.21213 62 51 1.203 1.2101 63 49 1.198 1.20756 64 53 1.196 1.20511 65 58 1.195 1.20238 66 55 1.193 1.1994 67 59 1.189 1.19673 68 51 1.187 1.19454 69 53 1.182 1.19158 70 45 1.18 1.18902 71 56 1.178 1.18627 72 47 1.178 1.18339 73 52 1.173 1.18156 74 53 1.17 1.17923 75 60 1.166 1.17717 76 63 1.166 1.17456 77 47 1.158 1.17134 78 56 1.158 1.16806 79 58 1.157 1.16517 80 52 1.155 1.16163 81 51 1.151 1.15874 82 53 1.147 1.15671 83 60 1.144 1.15379 84 52 1.142 1.15061 85 59 1.141 1.14821 86 54 1.135 1.14556 87 59 1.135 1.14202 88 62 1.131 1.1388 89 52 1.127 1.13583 90 49 1.127 1.13403 91 55 1.126 1.13146 92 63 1.124 1.12952 93 63 1.119 1.12797 94 51 1.119 1.12547 95 60 1.116 1.12368 96 54 1.115 1.12183 97 54 1.112 1.11971 98 61 1.104 1.11722 99 61 1.104 1.11505 100 60 1.104 1.11239 . train accuracy: 70.33492822966508 % test accuracy: 58.0 % . Result = model(toolbox, train_set_x, train_set_y, test_set_x, test_set_y) . gen nevals min avg 0 100 0.771 1.46543 1 58 0.771 0.9899 2 53 0.722 0.90938 3 61 0.706 0.87544 4 51 0.706 0.88109 5 56 0.675 0.89101 6 54 0.643 0.82273 7 58 0.643 0.78794 8 56 0.643 0.76938 9 59 0.643 0.72664 10 62 0.612 0.72597 11 59 0.612 0.67766 12 47 0.593 0.65913 13 63 0.593 0.68578 14 62 0.593 0.64169 15 60 0.589 0.62839 16 59 0.577 0.61321 17 61 0.568 0.60937 18 51 0.568 0.59778 19 54 0.563 0.5909 20 61 0.563 0.58798 21 53 0.553 0.58162 22 55 0.553 0.57064 23 60 0.539 0.57775 24 56 0.539 0.56588 25 53 0.539 0.56069 26 45 0.539 0.55624 27 53 0.532 0.55899 28 56 0.532 0.54795 29 58 0.527 0.54504 30 60 0.526 0.539 31 57 0.526 0.53629 32 59 0.524 0.53528 33 56 0.524 0.53071 34 58 0.524 0.52796 35 57 0.524 0.52631 36 57 0.52 0.526 37 61 0.52 0.52581 38 58 0.52 0.5235 39 46 0.518 0.52188 40 56 0.516 0.5209 41 54 0.515 0.51965 42 56 0.515 0.51881 43 56 0.514 0.51746 44 54 0.513 0.51606 45 53 0.512 0.51525 46 64 0.511 0.51456 47 52 0.51 0.51361 48 58 0.51 0.51294 49 49 0.51 0.51222 50 57 0.509 0.51209 51 56 0.508 0.51082 52 58 0.508 0.50997 53 56 0.507 0.50913 54 54 0.506 0.5083 55 49 0.505 0.50771 56 53 0.505 0.50717 57 56 0.504 0.50641 58 66 0.504 0.50562 59 53 0.503 0.50487 60 52 0.502 0.50441 61 54 0.502 0.50372 62 50 0.502 0.50303 63 50 0.501 0.50261 64 48 0.501 0.50212 65 56 0.501 0.50208 66 60 0.501 0.50178 67 51 0.5 0.50135 68 51 0.5 0.50107 69 58 0.5 0.50072 70 46 0.499 0.50028 71 46 0.499 0.50017 72 47 0.499 0.49997 73 55 0.498 0.49981 74 56 0.498 0.4994 75 59 0.498 0.49908 76 55 0.498 0.49886 77 59 0.498 0.49852 78 55 0.497 0.49819 79 52 0.497 0.49808 80 61 0.497 0.49808 81 53 0.496 0.49778 82 57 0.496 0.49734 83 54 0.496 0.49703 84 47 0.495 0.49668 85 59 0.495 0.49625 86 51 0.495 0.49597 87 65 0.495 0.4957 88 55 0.494 0.49523 89 50 0.494 0.49508 90 53 0.494 0.49494 91 59 0.493 0.4946 92 52 0.493 0.49413 93 50 0.493 0.49403 94 52 0.493 0.49396 95 55 0.493 0.49381 96 51 0.492 0.4934 97 55 0.492 0.49318 98 48 0.492 0.49308 99 51 0.492 0.49297 100 56 0.492 0.49277 . train accuracy: 76.55502392344498 % test accuracy: 66.0 % . Notes . The bounding range improves the GA choice of values since it is less space to search in. | From the two graphs we see that the min and average fitness converge early on approximately the Geneartion 40.We conclude that the diversity inside the population disapears quickly. | To keep the population diversified maybe change : P_MUTATION : increse probability of mutation of individual HALL_OF_FAME_SIZE : minimise number of best solutions passed | . Result = model(toolbox, train_set_x, train_set_y, test_set_x, test_set_y) . gen nevals min avg 0 100 0.771 1.46543 1 64 0.763 0.97667 2 68 0.714 0.97071 3 58 0.714 0.87276 4 64 0.691 0.80997 5 62 0.689 0.81111 6 71 0.669 0.81154 7 67 0.669 0.77923 8 66 0.627 0.7431 9 73 0.622 0.7175 10 64 0.61 0.74455 11 65 0.61 0.6892 12 68 0.602 0.66283 13 69 0.602 0.62883 14 68 0.596 0.6255 15 78 0.596 0.64238 16 66 0.583 0.62473 17 67 0.581 0.61421 18 72 0.562 0.62368 19 73 0.557 0.60044 20 73 0.557 0.59567 21 70 0.554 0.5994 22 64 0.545 0.58325 23 68 0.537 0.57174 24 69 0.536 0.57145 25 68 0.533 0.55576 26 66 0.527 0.54757 27 57 0.523 0.53688 28 66 0.523 0.53582 29 71 0.517 0.5304 30 67 0.517 0.5276 31 72 0.51 0.52447 32 70 0.51 0.52048 33 60 0.51 0.5167 34 70 0.507 0.51428 35 74 0.505 0.512 36 68 0.505 0.50968 37 75 0.503 0.50794 38 68 0.498 0.50746 39 75 0.497 0.50523 40 60 0.497 0.50297 41 65 0.496 0.50175 42 74 0.494 0.49945 43 67 0.494 0.49782 44 74 0.492 0.49641 45 66 0.492 0.49504 46 74 0.49 0.49405 47 69 0.489 0.49271 48 81 0.489 0.49185 49 71 0.488 0.49071 50 63 0.486 0.48982 51 67 0.486 0.48892 52 68 0.486 0.48856 53 65 0.485 0.4875 54 59 0.485 0.48676 55 67 0.484 0.48603 56 69 0.484 0.48534 57 64 0.483 0.48468 58 68 0.482 0.48447 59 64 0.482 0.4839 60 73 0.482 0.48339 61 60 0.481 0.48279 62 78 0.481 0.48233 63 80 0.48 0.48189 64 60 0.48 0.4813 65 70 0.48 0.48104 66 68 0.479 0.4807 67 68 0.479 0.48022 68 69 0.478 0.47995 69 70 0.478 0.47925 70 63 0.477 0.47872 71 64 0.477 0.47814 72 74 0.477 0.47809 73 67 0.477 0.47778 74 74 0.476 0.47737 75 56 0.476 0.47691 76 64 0.475 0.47651 77 65 0.475 0.47616 78 71 0.474 0.47588 79 60 0.474 0.47527 80 74 0.474 0.47495 81 73 0.473 0.47452 82 69 0.473 0.47416 83 59 0.472 0.47368 84 72 0.471 0.47304 85 69 0.471 0.47246 86 66 0.471 0.47208 87 76 0.471 0.47159 88 69 0.47 0.47123 89 69 0.47 0.47102 90 75 0.469 0.47074 91 67 0.469 0.47039 92 66 0.468 0.47013 93 66 0.468 0.46981 94 66 0.468 0.46926 95 75 0.468 0.46864 96 67 0.467 0.46822 97 69 0.466 0.46788 98 69 0.466 0.46748 99 63 0.466 0.46692 100 70 0.465 0.46654 . train accuracy: 81.33971291866028 % test accuracy: 54.0 % . Achieving more on the training set doesn&#39;t necessarly means that it will perform better on the test set | . Result = model(toolbox, train_set_x, train_set_y, test_set_x, test_set_y) . gen nevals min avg 0 100 0.771 1.46543 1 68 0.771 1.02503 2 66 0.73 0.95984 3 74 0.73 0.98152 4 67 0.726 0.9101 5 64 0.71 0.85006 6 65 0.679 0.8373 7 72 0.661 0.8359 8 68 0.661 0.79814 9 71 0.643 0.76927 10 68 0.621 0.78521 11 69 0.607 0.73339 12 70 0.586 0.73494 13 68 0.586 0.70797 14 73 0.572 0.67897 15 72 0.565 0.68997 16 71 0.565 0.6223 17 65 0.552 0.61038 18 66 0.552 0.60557 19 65 0.547 0.57607 20 66 0.542 0.57464 21 63 0.534 0.56496 22 65 0.53 0.5597 23 69 0.528 0.55846 24 67 0.518 0.54838 25 66 0.518 0.54234 26 69 0.511 0.54322 27 60 0.503 0.53506 28 65 0.498 0.52928 29 66 0.498 0.52709 30 73 0.498 0.52292 31 62 0.491 0.51743 32 71 0.491 0.51499 33 64 0.489 0.50427 34 74 0.484 0.50503 35 70 0.478 0.50212 36 65 0.472 0.49462 37 75 0.471 0.49488 38 69 0.471 0.48824 39 63 0.466 0.48483 40 69 0.463 0.48414 41 65 0.463 0.47885 42 67 0.462 0.47676 43 53 0.458 0.47309 44 62 0.457 0.47043 45 67 0.457 0.46631 46 74 0.456 0.46522 47 67 0.455 0.46522 48 63 0.447 0.46133 49 73 0.447 0.46205 50 72 0.447 0.45972 51 70 0.447 0.45905 52 73 0.445 0.45338 53 63 0.443 0.4546 54 68 0.442 0.45095 55 67 0.442 0.44906 56 71 0.44 0.44934 57 69 0.44 0.44705 58 66 0.439 0.44705 59 57 0.439 0.44548 60 72 0.438 0.44668 61 62 0.438 0.4442 62 65 0.434 0.44153 63 76 0.433 0.44232 64 66 0.431 0.43954 65 71 0.431 0.43894 66 70 0.431 0.43852 67 68 0.43 0.43904 68 68 0.429 0.43526 69 62 0.428 0.43527 70 55 0.427 0.43241 71 60 0.427 0.4315 72 71 0.423 0.43211 73 71 0.423 0.43158 74 65 0.421 0.42913 75 68 0.421 0.42934 76 73 0.42 0.42684 77 70 0.42 0.4257 78 66 0.418 0.42551 79 60 0.418 0.42458 80 66 0.417 0.42349 81 68 0.417 0.42213 82 68 0.416 0.42187 83 67 0.415 0.42067 84 67 0.414 0.4196 85 72 0.414 0.41916 86 66 0.411 0.4183 87 68 0.411 0.41718 88 68 0.41 0.41588 89 65 0.41 0.41751 90 68 0.409 0.41466 91 70 0.407 0.41332 92 65 0.407 0.41373 93 74 0.407 0.41294 94 73 0.405 0.4136 95 71 0.405 0.41075 96 70 0.404 0.41006 97 64 0.403 0.40766 98 71 0.403 0.40712 99 66 0.401 0.40755 100 61 0.399 0.40606 . train accuracy: 84.68899521531101 % test accuracy: 60.0 % . Notes . increasing the ipnb keeps the population diversified more since the mean and min not exactly the same through generations. | The slope of the graph after Geneartion 40 is largely negative due to the mutation aspect. | . Conclusion . GA is difficult to train , due to the high number of parameters and dimensions of search space, and with no guidance from the outside world but the value of the cost function. . | GA is suited for problems that are too complex to find a mathematical representation or where the loss function contains too many local minimums. In this problem, the backpropagation Algorithm is expected to outperform the GA, since the cost function is differentiable, and the weight update is performed intelligently with the calculation of gradients. . | A more suited approach is to combine both algorithms to benefit from the global optimisation aspect of the GA and prevent Backprop Algorithms in getting stucked in local minimums. . | .",
            "url": "https://elfaizamine.github.io/data_science_blog/fastpages/jupyter/2023/01/01/tt.html",
            "relUrl": "/fastpages/jupyter/2023/01/01/tt.html",
            "date": " • Jan 1, 2023"
        }
        
    
  
    
        ,"post1": {
            "title": "Introducing fastpages",
            "content": ". We are very pleased to announce the immediate availability of fastpages. fastpages is a platform which allows you to create and host a blog for free, with no ads and many useful features, such as: . Create posts containing code, outputs of code (which can be interactive), formatted text, etc directly from Jupyter Notebooks; for instance see this great example post from Scott Hawley. Notebook posts support features such as: Interactive visualizations made with Altair remain interactive. | Hide or show cell input and output. | Collapsable code cells that are either open or closed by default. | Define the Title, Summary and other metadata via a special markdown cells | Ability to add links to Colab and GitHub automatically. | . | Create posts, including formatting and images, directly from Microsoft Word documents. | Create and edit Markdown posts entirely online using GitHub&#39;s built-in markdown editor. | Embed Twitter cards and YouTube videos. | Categorization of blog posts by user-supplied tags for discoverability. | ... and much more | . fastpages relies on Github pages for hosting, and Github Actions to automate the creation of your blog. The setup takes around three minutes, and does not require any technical knowledge or expertise. Due to built-in automation of fastpages, you don&#39;t have to fuss with conversion scripts. All you have to do is save your Jupyter notebook, Word document or markdown file into a specified directory and the rest happens automatically. Infact, this blog post is written in a Jupyter notebook, which you can see with the &quot;View on GitHub&quot; link above. . fast.ai have previously released a similar project called fast_template, which is even easier to set up, but does not support automatic creation of posts from Microsoft Word or Jupyter notebooks, including many of the features outlined above. . Because fastpages is more flexible and extensible, we recommend using it where possible. fast_template may be a better option for getting folks blogging who have no technical expertise at all, and will only be creating posts using Github&#39;s integrated online editor. . Setting Up Fastpages . The setup process of fastpages is automated with GitHub Actions, too! Upon creating a repo from the fastpages template, a pull request will automatically be opened (after ~ 30 seconds) configuring your blog so it can start working. The automated pull request will greet you with instructions like this: . . All you have to do is follow these instructions (in the PR you receive) and your new blogging site will be up and running! . Jupyter Notebooks &amp; Fastpages . In this post, we will cover special features that fastpages provides for Jupyter notebooks. You can also write your blog posts with Word documents or markdown in fastpages, which contain many, but not all the same features. . Options via FrontMatter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # Title &gt; Awesome summary - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . All of the above settings are enabled in this post, so you can see what they look like! . the summary field (preceeded by &gt;) will be displayed under your title, and will also be used by social media to display as the description of your page. | toc: setting this to true will automatically generate a table of contents | badges: setting this to true will display Google Colab and GitHub links on your blog post. | comments: setting this to true will enable comments. See these instructions for more details. | author this will display the authors names. | categories will allow your post to be categorized on a &quot;Tags&quot; page, where readers can browse your post by categories. | . Markdown front matter is formatted similarly to notebooks. The differences between the two can be viewed on the fastpages README. . Code Folding . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . If you want to completely hide cells (not just collapse them), read these instructions. . Interactive Charts With Altair . Interactive visualizations made with Altair remain interactive! . We leave this below cell unhidden so you can enjoy a preview of syntax highlighting in fastpages, which uses the Dracula theme. . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;IMDB_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget IMDB_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | 6.1 | . 1 First Love, Last Rites | 10876.0 | 300000.0 | 6.9 | . 2 I Married a Strange Person | 203134.0 | 250000.0 | 6.8 | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | NaN | . 4 Slam | 1087521.0 | 1000000.0 | 3.4 | . Other Features . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Images w/Captions . You can include markdown images with captions like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Of course, the caption is optional. . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . More Examples . This tutorial contains more examples of what you can do with notebooks. . How fastpages Converts Notebooks to Blog Posts . fastpages uses nbdev to power the conversion process of Jupyter Notebooks to blog posts. When you save a notebook into the /_notebooks folder of your repository, GitHub Actions applies nbdev against those notebooks automatically. The same process occurs when you save Word documents or markdown files into the _word or _posts directory, respectively. . We will discuss how GitHub Actions work in a follow up blog post. . Resources &amp; Next Steps . We highly encourage you to start blogging with fastpages! Some resources that may be helpful: . fastpages repo - this is where you can go to create your own fastpages blog! | Fastai forums - nbdev &amp; blogging category. You can ask questions about fastpages here, as well as suggest new features. | nbdev: this project powers the conversion of Jupyter notebooks to blog posts. | . If you end up writing a blog post using fastpages, please let us know on Twitter: @jeremyphoward, @HamelHusain. .",
            "url": "https://elfaizamine.github.io/data_science_blog/fastpages/jupyter/2022/02/17/test.html",
            "relUrl": "/fastpages/jupyter/2022/02/17/test.html",
            "date": " • Feb 17, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Research: Can you train a Neural Network using Genetic Algorithm?",
            "content": "Context . The goal of this notebook is to asses the use of genetic algorithm in training neural network and updating the model weights and compare it with the classical way of using backpropagation algorithm. . Here are some information about the project: . Model : feed forward neural network | Dataset : Images of cats and non-cats | The backpropagation algorithm is developed from scratch and inspired by the deeplearning.ai course. | . Load Librairies and Data set . import matplotlib.pyplot as plt from load_data import load_dataset import backprop_functions as bp from PIL import Image import seaborn as sns import numpy as np import h5py import scipy import random %matplotlib inline import genetic algorithms library from deap import base from deap import creator from deap import tools import elitism from random import randint . Data understanding . train_set_x_orig, train_set_y, test_set_x_orig, test_set_y, classes = load_dataset() . index = 2 plt.imshow(train_set_x_orig[index]) print (&quot;The image with this index is labeld y = &quot; + str(train_set_y[:, index][0]) + &quot;, it&#39;s a &#39;&quot; + classes[np.squeeze(train_set_y[:, index])].decode(&quot;utf-8&quot;) + &quot;&#39; picture.&quot;) . The image with this index is labeld y = 1, it&#39;s a &#39;cat&#39; picture. . index = 3 plt.imshow(train_set_x_orig[index]) print (&quot;The image with this index is labeld y = &quot; + str(train_set_y[:, index][0]) + &quot;, it&#39;s a &#39;&quot; + classes[np.squeeze(train_set_y[:, index])].decode(&quot;utf-8&quot;) + &quot;&#39; picture.&quot;) . The image with this index is labeld y = 0, it&#39;s a &#39;non-cat&#39; picture. . m_train = train_set_x_orig.shape[0] m_test = test_set_x_orig.shape[0] num_px = train_set_x_orig.shape[1] print (&quot;Each image have a size of: (&quot; + str(num_px) + &quot;, &quot; + str(num_px) + &quot;, 3)&quot;) print (&quot;Number of training examples is: m_train = &quot; + str(m_train)) print (&quot;Number of testing examples is: m_test = &quot; + str(m_test)) print (&quot;The Height and Width of each image is: num_px = &quot; + str(num_px)) . Each image have a size of: (64, 64, 3) Number of training examples is: m_train = 209 Number of testing examples is: m_test = 50 The Height and Width of each image is: num_px = 64 . train_set_x_flatten = train_set_x_orig.reshape(train_set_x_orig.shape[0], -1).T test_set_x_flatten = test_set_x_orig.reshape(test_set_x_orig.shape[0], -1).T # Standarize the inputs train_set_x = train_set_x_flatten/255. test_set_x = test_set_x_flatten/255. print (&quot;train_set_x_flatten shape: &quot; + str(train_set_x_flatten.shape)) print (&quot;train_set_y shape: &quot; + str(train_set_y.shape)) print (&quot;test_set_x_flatten shape: &quot; + str(test_set_x_flatten.shape)) print (&quot;test_set_y shape: &quot; + str(test_set_y.shape)) . train_set_x_flatten shape: (12288, 209) train_set_y shape: (1, 209) test_set_x_flatten shape: (12288, 50) test_set_y shape: (1, 50) . BackPropagation . This BackPropagation part is taken from DeepLearning.ai Course . def model(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.5, print_cost = False): &quot;&quot;&quot; Builds the logistic regression model by calling the function you&#39;ve implemented previously Arguments: X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train) Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train) X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test) Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test) num_iterations -- hyperparameter representing the number of iterations to optimize the parameters learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize() print_cost -- Set to true to print the cost every 100 iterations Returns: d -- dictionary containing information about the model. &quot;&quot;&quot; # initialize parameters with zeros w, b = bp.initialize_with_zeros(X_train.shape[0]) # Gradient descent parameters, grads, costs = bp.optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost) # Retrieve parameters w and b from dictionary &quot;parameters&quot; w = parameters[&quot;w&quot;] b = parameters[&quot;b&quot;] # Predict test/train set examples Y_prediction_test = bp.predict(w, b, X_test) Y_prediction_train = bp.predict(w, b, X_train) ### END CODE HERE ### # Print train/test Errors print(&quot;train accuracy: {} %&quot;.format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100)) print(&quot;test accuracy: {} %&quot;.format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100)) d = {&quot;costs&quot;: costs, &quot;Y_prediction_test&quot;: Y_prediction_test, &quot;Y_prediction_train&quot; : Y_prediction_train, &quot;w&quot; : w, &quot;b&quot; : b, &quot;learning_rate&quot; : learning_rate, &quot;num_iterations&quot;: num_iterations} return d . result = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = 2000, learning_rate = 0.005, print_cost = True) . Cost after iteration 0: 0.693147 Cost after iteration 100: 0.584508 Cost after iteration 200: 0.466949 Cost after iteration 300: 0.376007 Cost after iteration 400: 0.331463 Cost after iteration 500: 0.303273 Cost after iteration 600: 0.279880 Cost after iteration 700: 0.260042 Cost after iteration 800: 0.242941 Cost after iteration 900: 0.228004 Cost after iteration 1000: 0.214820 Cost after iteration 1100: 0.203078 Cost after iteration 1200: 0.192544 Cost after iteration 1300: 0.183033 Cost after iteration 1400: 0.174399 Cost after iteration 1500: 0.166521 Cost after iteration 1600: 0.159305 Cost after iteration 1700: 0.152667 Cost after iteration 1800: 0.146542 Cost after iteration 1900: 0.140872 train accuracy: 99.04306220095694 % test accuracy: 70.0 % . Genetic Algorithm . In this part I am going to train a one layer neural network using genetic algorithm. In other words, finding the optimal set of values &quot;Weight Vector&quot; that will minimize the cost function simply through iteration and Fitness values. . The chromosome representation of the GA will the the weight vector itself of the Neural Network. | We will start with a population of chromosome (potential solutions) of POPULATION_SIZE. | Apply a selection process &quot;Tournament selection&quot; to choose the best parents of the next generation. | Apply a crossover process &quot;SimulatedBinaryBounded&quot; within boundaries BOUND_LOW, BOUND_UP | Apply a mutation process &quot;mutPolynomialBounded&quot; within boundaries BOUND_LOW, BOUND_UP | Use elitism to pass the x best solution fom generation G to G+1 with no modification. | Repeat the process until a stopping condition is reached : here is MAX_GENERATIONS. | . def randomFloat(low , up): return [random.uniform(l, u) for l, u in zip([low] * (DIMENSIONS+1), [up] * (DIMENSIONS+1))] # Fitness function : score that will let the GA knows wich solution to choose to be the parents of the next generation def cross_entropy_loss(individual, X_train=train_set_x, Y_train=train_set_y) : eps = 1e-5 m = X_train.shape[1] w = np.array(individual[:-1]) b = individual[-1] A = bp.sigmoid(np.dot(w.T,X_train)+b) cost = round(-(1/m) * np.sum(Y_train*np.log(A+eps)+(1-Y_train)*np.log(1-A+eps)),3) , return cost . DIMENSIONS = train_set_x.shape[0] # Length of chromosome thar reprensent the weights BOUND_LOW, BOUND_UP = -0.1, 0.1 # Boundaries of weight values POPULATION_SIZE = 100 # Population size P_CROSSOVER = 0.7 # probability for crossover P_MUTATION = 0.3 # probability for mutating an individual MAX_GENERATIONS = 100 # max generations to ierate HALL_OF_FAME_SIZE = 20 # Number of individuals to pass from G to G+1 without crossover nor mutation CROWDING_FACTOR = 10.0 # crowding factor for crossover and mutation . random.seed(42) # Definition of algorithms to compute the GA pipeline using DEAP toolbox = base.Toolbox() creator.create(&quot;FitnessMin&quot;, base.Fitness, weights=(-1.0,)) creator.create(&quot;Individual&quot;, list, fitness=creator.FitnessMin) toolbox.register(&quot;attrFloat&quot;, randomFloat, BOUND_LOW, BOUND_UP) toolbox.register(&quot;individualCreator&quot;, tools.initIterate, creator.Individual, toolbox.attrFloat) toolbox.register(&quot;populationCreator&quot;, tools.initRepeat, list, toolbox.individualCreator) toolbox.register(&quot;evaluate&quot;, cross_entropy_loss) toolbox.register(&quot;select&quot;, tools.selTournament,tournsize = 4) toolbox.register(&quot;mate&quot;, tools.cxSimulatedBinaryBounded, low=BOUND_LOW, up=BOUND_UP, eta=CROWDING_FACTOR) toolbox.register(&quot;mutate&quot;, tools.mutPolynomialBounded, low=BOUND_LOW, up=BOUND_UP, eta=CROWDING_FACTOR, indpb=10.0/DIMENSIONS) . def model(toolbox, X_train, Y_train, X_test, Y_test): &quot;&quot;&quot; Builds the logistic regression model by calling the function you&#39;ve implemented previously but training with GA algorithm Arguments: X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train) Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train) X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test) Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test) toolbox -- GA class that saves optimisation parameters and algorithms to be used later Returns: d -- dictionary containing information about the model. &quot;&quot;&quot; population = toolbox.populationCreator(n=POPULATION_SIZE) # prepare the statistics object: stats = tools.Statistics(lambda ind: ind.fitness.values) stats.register(&quot;min&quot;, np.min) stats.register(&quot;avg&quot;, np.mean) # define the hall-of-fame object: hof = tools.HallOfFame(HALL_OF_FAME_SIZE) # perform the Genetic Algorithm flow with elitism: population, logbook = elitism.eaSimpleWithElitism(population, toolbox, cxpb=P_CROSSOVER, mutpb=P_MUTATION, ngen=MAX_GENERATIONS, stats=stats, halloffame=hof, verbose=True) # Infos on best solution found: best = hof.items[0] # extract statistics: minFitnessValues, meanFitnessValues = logbook.select(&quot;min&quot;, &quot;avg&quot;) # plot statistics: sns.set_style(&quot;whitegrid&quot;) plt.plot(minFitnessValues, color=&#39;red&#39;) plt.plot(meanFitnessValues, color=&#39;green&#39;) plt.xlabel(&#39;Generation&#39;) plt.ylabel(&#39;Min / Average Fitness&#39;) plt.title(&#39;Min and Average fitness over Generations&#39;) plt.show() # Retrieve parameters w and b from dictionary &quot;parameters&quot; w = np.array(best[:-1]) b = best[-1] # Predict test/train Y_prediction_test = bp.predict(w, b, X_test) Y_prediction_train = bp.predict(w, b, X_train) print(&quot;train accuracy: {} %&quot;.format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100)) print(&quot;test accuracy: {} %&quot;.format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100)) d = {&quot;costs&quot;: best.fitness.values[0], &quot;Y_prediction_test&quot;: Y_prediction_test, &quot;Y_prediction_train&quot; : Y_prediction_train, &quot;w&quot; : w, &quot;b&quot; : b} return d . Result = model(toolbox, train_set_x, train_set_y, test_set_x, test_set_y) . gen nevals min avg 0 100 2.241 3.94754 1 58 2.211 2.99987 2 65 1.789 2.83352 3 55 1.789 2.60072 4 55 1.602 2.42983 5 62 1.602 2.33102 6 52 1.602 2.37952 7 57 1.539 2.04481 8 51 1.539 2.04603 9 58 1.539 2.12531 10 61 1.403 1.9389 11 60 1.403 1.95289 12 51 1.403 1.83036 13 61 1.384 1.86647 14 58 1.376 1.56289 15 55 1.349 1.4422 16 54 1.344 1.39843 17 54 1.344 1.37828 18 54 1.336 1.36181 19 58 1.329 1.3518 20 47 1.326 1.34517 21 57 1.326 1.34066 22 56 1.326 1.33443 23 58 1.316 1.3306 24 56 1.313 1.32599 25 50 1.311 1.32098 26 58 1.308 1.31675 27 57 1.307 1.31374 28 54 1.303 1.31182 29 53 1.297 1.30938 30 50 1.296 1.30667 31 52 1.294 1.30319 32 54 1.291 1.29944 33 56 1.284 1.29604 34 56 1.279 1.29231 35 51 1.279 1.28843 36 56 1.273 1.28546 37 61 1.273 1.28224 38 52 1.27 1.27894 39 54 1.258 1.27544 40 57 1.258 1.27193 41 58 1.256 1.26717 42 56 1.253 1.26292 43 59 1.252 1.25916 44 58 1.247 1.25704 45 56 1.247 1.25423 46 60 1.241 1.25164 47 55 1.241 1.24831 48 56 1.236 1.24513 49 56 1.235 1.24251 50 56 1.231 1.2396 51 57 1.227 1.23698 52 51 1.225 1.23418 53 54 1.225 1.23115 54 61 1.221 1.22855 55 54 1.22 1.2256 56 55 1.217 1.22441 57 55 1.213 1.2211 58 54 1.213 1.21911 59 54 1.209 1.21672 60 58 1.207 1.21456 61 55 1.205 1.21213 62 51 1.203 1.2101 63 49 1.198 1.20756 64 53 1.196 1.20511 65 58 1.195 1.20238 66 55 1.193 1.1994 67 59 1.189 1.19673 68 51 1.187 1.19454 69 53 1.182 1.19158 70 45 1.18 1.18902 71 56 1.178 1.18627 72 47 1.178 1.18339 73 52 1.173 1.18156 74 53 1.17 1.17923 75 60 1.166 1.17717 76 63 1.166 1.17456 77 47 1.158 1.17134 78 56 1.158 1.16806 79 58 1.157 1.16517 80 52 1.155 1.16163 81 51 1.151 1.15874 82 53 1.147 1.15671 83 60 1.144 1.15379 84 52 1.142 1.15061 85 59 1.141 1.14821 86 54 1.135 1.14556 87 59 1.135 1.14202 88 62 1.131 1.1388 89 52 1.127 1.13583 90 49 1.127 1.13403 91 55 1.126 1.13146 92 63 1.124 1.12952 93 63 1.119 1.12797 94 51 1.119 1.12547 95 60 1.116 1.12368 96 54 1.115 1.12183 97 54 1.112 1.11971 98 61 1.104 1.11722 99 61 1.104 1.11505 100 60 1.104 1.11239 . train accuracy: 70.33492822966508 % test accuracy: 58.0 % . Result = model(toolbox, train_set_x, train_set_y, test_set_x, test_set_y) . gen nevals min avg 0 100 0.771 1.46543 1 58 0.771 0.9899 2 53 0.722 0.90938 3 61 0.706 0.87544 4 51 0.706 0.88109 5 56 0.675 0.89101 6 54 0.643 0.82273 7 58 0.643 0.78794 8 56 0.643 0.76938 9 59 0.643 0.72664 10 62 0.612 0.72597 11 59 0.612 0.67766 12 47 0.593 0.65913 13 63 0.593 0.68578 14 62 0.593 0.64169 15 60 0.589 0.62839 16 59 0.577 0.61321 17 61 0.568 0.60937 18 51 0.568 0.59778 19 54 0.563 0.5909 20 61 0.563 0.58798 21 53 0.553 0.58162 22 55 0.553 0.57064 23 60 0.539 0.57775 24 56 0.539 0.56588 25 53 0.539 0.56069 26 45 0.539 0.55624 27 53 0.532 0.55899 28 56 0.532 0.54795 29 58 0.527 0.54504 30 60 0.526 0.539 31 57 0.526 0.53629 32 59 0.524 0.53528 33 56 0.524 0.53071 34 58 0.524 0.52796 35 57 0.524 0.52631 36 57 0.52 0.526 37 61 0.52 0.52581 38 58 0.52 0.5235 39 46 0.518 0.52188 40 56 0.516 0.5209 41 54 0.515 0.51965 42 56 0.515 0.51881 43 56 0.514 0.51746 44 54 0.513 0.51606 45 53 0.512 0.51525 46 64 0.511 0.51456 47 52 0.51 0.51361 48 58 0.51 0.51294 49 49 0.51 0.51222 50 57 0.509 0.51209 51 56 0.508 0.51082 52 58 0.508 0.50997 53 56 0.507 0.50913 54 54 0.506 0.5083 55 49 0.505 0.50771 56 53 0.505 0.50717 57 56 0.504 0.50641 58 66 0.504 0.50562 59 53 0.503 0.50487 60 52 0.502 0.50441 61 54 0.502 0.50372 62 50 0.502 0.50303 63 50 0.501 0.50261 64 48 0.501 0.50212 65 56 0.501 0.50208 66 60 0.501 0.50178 67 51 0.5 0.50135 68 51 0.5 0.50107 69 58 0.5 0.50072 70 46 0.499 0.50028 71 46 0.499 0.50017 72 47 0.499 0.49997 73 55 0.498 0.49981 74 56 0.498 0.4994 75 59 0.498 0.49908 76 55 0.498 0.49886 77 59 0.498 0.49852 78 55 0.497 0.49819 79 52 0.497 0.49808 80 61 0.497 0.49808 81 53 0.496 0.49778 82 57 0.496 0.49734 83 54 0.496 0.49703 84 47 0.495 0.49668 85 59 0.495 0.49625 86 51 0.495 0.49597 87 65 0.495 0.4957 88 55 0.494 0.49523 89 50 0.494 0.49508 90 53 0.494 0.49494 91 59 0.493 0.4946 92 52 0.493 0.49413 93 50 0.493 0.49403 94 52 0.493 0.49396 95 55 0.493 0.49381 96 51 0.492 0.4934 97 55 0.492 0.49318 98 48 0.492 0.49308 99 51 0.492 0.49297 100 56 0.492 0.49277 . train accuracy: 76.55502392344498 % test accuracy: 66.0 % . Notes . The bounding range improves the GA choice of values since it is less space to search in. | From the two graphs we see that the min and average fitness converge early on approximately the Geneartion 40.We conclude that the diversity inside the population disapears quickly. | To keep the population diversified maybe change : P_MUTATION : increse probability of mutation of individual HALL_OF_FAME_SIZE : minimise number of best solutions passed | . Result = model(toolbox, train_set_x, train_set_y, test_set_x, test_set_y) . gen nevals min avg 0 100 0.771 1.46543 1 64 0.763 0.97667 2 68 0.714 0.97071 3 58 0.714 0.87276 4 64 0.691 0.80997 5 62 0.689 0.81111 6 71 0.669 0.81154 7 67 0.669 0.77923 8 66 0.627 0.7431 9 73 0.622 0.7175 10 64 0.61 0.74455 11 65 0.61 0.6892 12 68 0.602 0.66283 13 69 0.602 0.62883 14 68 0.596 0.6255 15 78 0.596 0.64238 16 66 0.583 0.62473 17 67 0.581 0.61421 18 72 0.562 0.62368 19 73 0.557 0.60044 20 73 0.557 0.59567 21 70 0.554 0.5994 22 64 0.545 0.58325 23 68 0.537 0.57174 24 69 0.536 0.57145 25 68 0.533 0.55576 26 66 0.527 0.54757 27 57 0.523 0.53688 28 66 0.523 0.53582 29 71 0.517 0.5304 30 67 0.517 0.5276 31 72 0.51 0.52447 32 70 0.51 0.52048 33 60 0.51 0.5167 34 70 0.507 0.51428 35 74 0.505 0.512 36 68 0.505 0.50968 37 75 0.503 0.50794 38 68 0.498 0.50746 39 75 0.497 0.50523 40 60 0.497 0.50297 41 65 0.496 0.50175 42 74 0.494 0.49945 43 67 0.494 0.49782 44 74 0.492 0.49641 45 66 0.492 0.49504 46 74 0.49 0.49405 47 69 0.489 0.49271 48 81 0.489 0.49185 49 71 0.488 0.49071 50 63 0.486 0.48982 51 67 0.486 0.48892 52 68 0.486 0.48856 53 65 0.485 0.4875 54 59 0.485 0.48676 55 67 0.484 0.48603 56 69 0.484 0.48534 57 64 0.483 0.48468 58 68 0.482 0.48447 59 64 0.482 0.4839 60 73 0.482 0.48339 61 60 0.481 0.48279 62 78 0.481 0.48233 63 80 0.48 0.48189 64 60 0.48 0.4813 65 70 0.48 0.48104 66 68 0.479 0.4807 67 68 0.479 0.48022 68 69 0.478 0.47995 69 70 0.478 0.47925 70 63 0.477 0.47872 71 64 0.477 0.47814 72 74 0.477 0.47809 73 67 0.477 0.47778 74 74 0.476 0.47737 75 56 0.476 0.47691 76 64 0.475 0.47651 77 65 0.475 0.47616 78 71 0.474 0.47588 79 60 0.474 0.47527 80 74 0.474 0.47495 81 73 0.473 0.47452 82 69 0.473 0.47416 83 59 0.472 0.47368 84 72 0.471 0.47304 85 69 0.471 0.47246 86 66 0.471 0.47208 87 76 0.471 0.47159 88 69 0.47 0.47123 89 69 0.47 0.47102 90 75 0.469 0.47074 91 67 0.469 0.47039 92 66 0.468 0.47013 93 66 0.468 0.46981 94 66 0.468 0.46926 95 75 0.468 0.46864 96 67 0.467 0.46822 97 69 0.466 0.46788 98 69 0.466 0.46748 99 63 0.466 0.46692 100 70 0.465 0.46654 . train accuracy: 81.33971291866028 % test accuracy: 54.0 % . Achieving more on the training set doesn&#39;t necessarly means that it will perform better on the test set | . Result = model(toolbox, train_set_x, train_set_y, test_set_x, test_set_y) . gen nevals min avg 0 100 0.771 1.46543 1 68 0.771 1.02503 2 66 0.73 0.95984 3 74 0.73 0.98152 4 67 0.726 0.9101 5 64 0.71 0.85006 6 65 0.679 0.8373 7 72 0.661 0.8359 8 68 0.661 0.79814 9 71 0.643 0.76927 10 68 0.621 0.78521 11 69 0.607 0.73339 12 70 0.586 0.73494 13 68 0.586 0.70797 14 73 0.572 0.67897 15 72 0.565 0.68997 16 71 0.565 0.6223 17 65 0.552 0.61038 18 66 0.552 0.60557 19 65 0.547 0.57607 20 66 0.542 0.57464 21 63 0.534 0.56496 22 65 0.53 0.5597 23 69 0.528 0.55846 24 67 0.518 0.54838 25 66 0.518 0.54234 26 69 0.511 0.54322 27 60 0.503 0.53506 28 65 0.498 0.52928 29 66 0.498 0.52709 30 73 0.498 0.52292 31 62 0.491 0.51743 32 71 0.491 0.51499 33 64 0.489 0.50427 34 74 0.484 0.50503 35 70 0.478 0.50212 36 65 0.472 0.49462 37 75 0.471 0.49488 38 69 0.471 0.48824 39 63 0.466 0.48483 40 69 0.463 0.48414 41 65 0.463 0.47885 42 67 0.462 0.47676 43 53 0.458 0.47309 44 62 0.457 0.47043 45 67 0.457 0.46631 46 74 0.456 0.46522 47 67 0.455 0.46522 48 63 0.447 0.46133 49 73 0.447 0.46205 50 72 0.447 0.45972 51 70 0.447 0.45905 52 73 0.445 0.45338 53 63 0.443 0.4546 54 68 0.442 0.45095 55 67 0.442 0.44906 56 71 0.44 0.44934 57 69 0.44 0.44705 58 66 0.439 0.44705 59 57 0.439 0.44548 60 72 0.438 0.44668 61 62 0.438 0.4442 62 65 0.434 0.44153 63 76 0.433 0.44232 64 66 0.431 0.43954 65 71 0.431 0.43894 66 70 0.431 0.43852 67 68 0.43 0.43904 68 68 0.429 0.43526 69 62 0.428 0.43527 70 55 0.427 0.43241 71 60 0.427 0.4315 72 71 0.423 0.43211 73 71 0.423 0.43158 74 65 0.421 0.42913 75 68 0.421 0.42934 76 73 0.42 0.42684 77 70 0.42 0.4257 78 66 0.418 0.42551 79 60 0.418 0.42458 80 66 0.417 0.42349 81 68 0.417 0.42213 82 68 0.416 0.42187 83 67 0.415 0.42067 84 67 0.414 0.4196 85 72 0.414 0.41916 86 66 0.411 0.4183 87 68 0.411 0.41718 88 68 0.41 0.41588 89 65 0.41 0.41751 90 68 0.409 0.41466 91 70 0.407 0.41332 92 65 0.407 0.41373 93 74 0.407 0.41294 94 73 0.405 0.4136 95 71 0.405 0.41075 96 70 0.404 0.41006 97 64 0.403 0.40766 98 71 0.403 0.40712 99 66 0.401 0.40755 100 61 0.399 0.40606 . train accuracy: 84.68899521531101 % test accuracy: 60.0 % . Notes . increasing the ipnb keeps the population diversified more since the mean and min not exactly the same through generations. | The slope of the graph after Geneartion 40 is largely negative due to the mutation aspect. | . Conclusion . GA is difficult to train , due to the high number of parameters and dimensions of search space, and with no guidance from the outside world but the value of the cost function. . | GA is suited for problems that are too complex to find a mathematical representation or where the loss function contains too many local minimums. In this problem, the backpropagation Algorithm is expected to outperform the GA, since the cost function is differentiable, and the weight update is performed intelligently with the calculation of gradients. . | A more suited approach is to combine both algorithms to benefit from the global optimisation aspect of the GA and prevent Backprop Algorithms in getting stucked in local minimums. . | .",
            "url": "https://elfaizamine.github.io/data_science_blog/jupyter/2020/05/01/genetic.html",
            "relUrl": "/jupyter/2020/05/01/genetic.html",
            "date": " • May 1, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://elfaizamine.github.io/data_science_blog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://elfaizamine.github.io/data_science_blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://elfaizamine.github.io/data_science_blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}