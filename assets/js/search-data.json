{
  
    
        "post0": {
            "title": "Feature Engineering: Website Categorization",
            "content": "Context . The goal of this project is websites categorization which refers to the process of classifying websites into various categories based on their content and purpose. For example amazon website can be classified into e-commerce category. . Website categorization either manually or by using machine learning algorithms can be useful in many areas: . Content filtering: blocking certain websites dependent on their categories from access by certain users. | Contextual Marketing: Allows businesses to display ads on pages that are similar or relevant to the product or service they offer. | Brand protection: Looking for copycat websites that are similar to yours but harm your brand. | Text Data Encoding: Transform websites text data into vectors that have meaning like a website embedding vector and websites with similar purpose have close vectors. | . I have come across this project when trying to encode websites text data into vectors in order to train a machine learning model on those vectors.One of the ways is to classify each website into a set of categories either by probability or classes. The advantage of using this method besides the vectorization is : . we can sum up multiple vectors and the result would meaningful for example in case of a user visdited a series of websites and we want to encode this sequence of visits. | if a machine learning model is trained on these categories we can interpret the results by doing feature importance. | . To obtain this objective the process of transformation is as follows: . Scrap from websites urls text data. | Define the categories that will define the content of each website. | Predict the probability that the scraped text belongs to each of the defined categories using pretrained Bart Model. | . Load Librairies . from transformers import AutoModel, AutoTokenizer import cloudscraper from bs4 import BeautifulSoup from googletrans import Translator from tldextract import extract import transformers from transformers import pipeline from transformers import AutoModel import pandas as pd import numpy as np import itertools from collections import Counter from googletrans import Translator import numpy as np import pandas as pd from bs4 import BeautifulSoup from tldextract import extract import re import warnings warnings.filterwarnings(&#39;ignore&#39;) . Websites Scraping . # Function that will scrapp a desired website def scrap_website(scraper, headers, website_name, text_size): allthecontent = &#39;&#39; try: r = scraper.get(website_name, headers=headers) soup = BeautifulSoup(r.text, &#39;html.parser&#39;) title = soup.find(&#39;title&#39;).text description = soup.find(&#39;meta&#39;, attrs={&#39;name&#39;: &#39;description&#39;}) if &quot;content&quot; in str(description): description = description.get(&quot;content&quot;) else: description = &quot;&quot; h1 = soup.find_all(&#39;h1&#39;) h1_all = &quot;&quot; for x in range(len(h1)): if x == len(h1) - 1: h1_all = h1_all + h1[x].text else: h1_all = h1_all + h1[x].text + &quot;. &quot; paragraphs_all = &quot;&quot; paragraphs = soup.find_all(&#39;p&#39;) for x in range(len(paragraphs)): if x == len(paragraphs) - 1: paragraphs_all = paragraphs_all + paragraphs[x].text else: paragraphs_all = paragraphs_all + paragraphs[x].text + &quot;. &quot; h2 = soup.find_all(&#39;h2&#39;) h2_all = &quot;&quot; for x in range(len(h2)): if x == len(h2) - 1: h2_all = h2_all + h2[x].text else: h2_all = h2_all + h2[x].text + &quot;. &quot; h3 = soup.find_all(&#39;h3&#39;) h3_all = &quot;&quot; for x in range(len(h3)): if x == len(h3) - 1: h3_all = h3_all + h3[x].text else: h3_all = h3_all + h3[x].text + &quot;. &quot; allthecontent = str(title) + &quot; &quot; + str(description) + &quot; &quot; + str(h1_all) + &quot; &quot; + str(h2_all) + &quot; &quot; + str( h3_all) + &quot; &quot; + str(paragraphs_all) allthecontent = str(allthecontent)[0:text_size] except Exception as e: pass return allthecontent def translate_sentence(translator, sentence, text_size): translation = translator.translate(sentence) translation = str(translation)[0:text_size] return translation . . # Scarp websites and translate words for non english websites scraper = cloudscraper.create_scraper() headers = {&#39;user-agent&#39;: &#39;Mozilla/5.0 (Linux; Android 6.0.1; Nexus 5X Build/MMB29P) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/85.0.4183.121 Mobile Safari/537.36 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)&#39;} translator = Translator() # Example of Amazon website_name = &#39;https://www.amazon.com&#39; website_content = scrap_website(scraper, headers, website_name, text_size = 700) website_content_en = translate_sentence(translator, website_content, text_size= 700) website_content_en = &#39; &#39;.join(re.findall(&#39;[a-z]+&#39;, website_content.lower())) print(&#39;The Scraped Text after cleaning and english translation of:&#39;, website_name, &#39;is:&#39;,website_content) # Example of youtube website_name = &#39;https://www.youtube.com&#39; website_content = scrap_website(scraper, headers, website_name, text_size = 700) website_content_en = translate_sentence(translator, website_content, text_size = 700) website_content_en = &#39; &#39;.join(re.findall(&#39;[a-z]+&#39;, website_content.lower())) print(&#39;The Scraped Text after cleaning and english translation of:&#39;, website_name, &#39;is:&#39;,website_content) . The Scraped Text after cleaning and english translation of: https://www.amazon.com is: Amazon.com. Spend less. Smile more. Free shipping on millions of items. Get the best of Shopping and Entertainment with Prime. Enjoy low prices and great deals on the largest selection of everyday essentials and other products, including fashion, home, beauty, electronics, Alexa Devices, sporting goods, toys, automotive, pets, baby, books, video games, musical instruments, office supplies, and more. Sign in for the best experience. Explore Departments The Scraped Text after cleaning and english translation of: https://www.youtube.com is: YouTube Share your videos with friends, family, and the world . Input and Output Definition . We will define the categories that the bart model will choose from and the websites we want to classify: . # websites to classify websites = [&#39;amazon.com&#39;,&#39;instagram.com&#39;,&#39;wikipedia.org&#39;,&#39;netflix.com&#39;,&#39;facebook.com&#39;,&#39;google.com&#39;,&#39;yahoo.com&#39;] # Categories from bart model to choose from and to labelize each website base on front page content candidate_labels = [&#39;health&#39;,&#39;e-commerce&#39;,&#39;advertising&#39;, &#39;job&#39;,&#39;computer&#39;,&#39;education&#39;,&#39;entertainment&#39;, &#39;home and family&#39;,&#39;industry&#39;,&#39;Information Technology&#39;,&#39;search engine&#39;, &#39;social network&#39;,&#39;science&#39;,&#39;news and media&#39;,&#39;read&#39;, &#39;buisness&#39;] # Tabular DataFrame containing vectors web2vec = pd.DataFrame(websites, columns=[&#39;website_url&#39;]) web2vec[candidate_labels] = np.nan . Categorization using BART classifier . For further learning about the facebook Bart Classification Model, refer to this link. . # Initialise Bart model that will transform website to useful encoding # Needs Internet Connection Must be downloaded before model_name = &quot;facebook/bart-large-mnli&quot; # Download pytorch model model = AutoModel.from_pretrained(model_name) # Load Bert Zero shot classification model classifier = pipeline(&quot;zero-shot-classification&quot;, model=&quot;facebook/bart-large-mnli&quot;) . Some weights of the model checkpoint at facebook/bart-large-mnli were not used when initializing BartModel: [&#39;classification_head.out_proj.weight&#39;, &#39;classification_head.dense.bias&#39;, &#39;classification_head.out_proj.bias&#39;, &#39;classification_head.dense.weight&#39;] - This IS expected if you are initializing BartModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model). - This IS NOT expected if you are initializing BartModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model). . . # Categorization Prediction of amazon website website_name = &#39;https://www.amazon.com&#39; website_content = scrap_website(scraper, headers, website_name, text_size = 700) website_content_en = translate_sentence(translator, website_content, text_size= 700) website_content_en = &#39; &#39;.join(re.findall(&#39;[a-z]+&#39;, website_content.lower())) output = classifier(website_content_en, candidate_labels) print(output) # The top probability class prediction is e-commerce which is correct. . {&#39;sequence&#39;: &#39;amazon com spend less smile more free shipping on millions of items get the best of shopping and entertainment with prime enjoy low prices and great deals on the largest selection of everyday essentials and other products including fashion home beauty electronics alexa devices sporting goods toys automotive pets baby books video games musical instruments office supplies and more sign in for the best experience explore departments&#39;, &#39;labels&#39;: [&#39;e-commerce&#39;, &#39;entertainment&#39;, &#39;read&#39;, &#39;buisness&#39;, &#39;home and family&#39;, &#39;industry&#39;, &#39;search engine&#39;, &#39;advertising&#39;, &#39;job&#39;, &#39;computer&#39;, &#39;health&#39;, &#39;social network&#39;, &#39;science&#39;, &#39;Information Technology&#39;, &#39;education&#39;, &#39;news and media&#39;], &#39;scores&#39;: [0.601190447807312, 0.1486208140850067, 0.09820207953453064, 0.037607591599226, 0.033550821244716644, 0.02008289285004139, 0.0157319363206625, 0.014724561013281345, 0.008524124510586262, 0.006259399000555277, 0.0036483018193393946, 0.0029156305827200413, 0.002454340225085616, 0.002257096115499735, 0.0021261561196297407, 0.0021038008853793144]} . # the encode_websites function group all steps defined earlier to makje a classification # Apply the pipeline on websites defined earlier webList = web2vec.website_url.to_list() for i in range(web2vec.shape[0]): print(&#39;Website url&#39;, webList[i], &#39;is being classified&#39;) try: output = encode_websites(classifier, scraper, headers,translator, website_name = webList[i], text_size = 500) if output != 0: web2vec.loc[i,output[&#39;labels&#39;][:3]] = output[&#39;scores&#39;][:3] else: pass except Exception as e: pass . Website url amazon.com is being classified Website url instagram.com is being classified Website url wikipedia.org is being classified Website url netflix.com is being classified Website url facebook.com is being classified Website url google.com is being classified Website url yahoo.com is being classified . web2vec.fillna(0, inplace=True) web2vec[candidate_labels] = round(web2vec[candidate_labels], 3) web2vec[&#39;Category_Prediction&#39;] = web2vec[candidate_labels].idxmax(axis=1) web2vec . website_url health e-commerce advertising job computer education entertainment home and family industry Information Technology search engine social network science news and media read buisness Category_Prediction . 0 amazon.com | 0.0 | 0.601 | 0.0 | 0.0 | 0.000 | 0.0 | 0.149 | 0.0 | 0.0 | 0.000 | 0.000 | 0.000 | 0.0 | 0.000 | 0.098 | 0.000 | e-commerce | . 1 instagram.com | 0.0 | 0.000 | 0.0 | 0.0 | 0.000 | 0.0 | 0.061 | 0.0 | 0.0 | 0.038 | 0.000 | 0.820 | 0.0 | 0.000 | 0.000 | 0.000 | social network | . 2 wikipedia.org | 0.0 | 0.000 | 0.0 | 0.0 | 0.141 | 0.0 | 0.000 | 0.0 | 0.0 | 0.000 | 0.000 | 0.000 | 0.0 | 0.000 | 0.304 | 0.102 | read | . 3 netflix.com | 0.0 | 0.000 | 0.0 | 0.0 | 0.000 | 0.0 | 0.296 | 0.0 | 0.0 | 0.169 | 0.000 | 0.000 | 0.0 | 0.000 | 0.180 | 0.000 | entertainment | . 4 facebook.com | 0.0 | 0.000 | 0.0 | 0.0 | 0.000 | 0.0 | 0.000 | 0.0 | 0.0 | 0.070 | 0.000 | 0.767 | 0.0 | 0.000 | 0.071 | 0.000 | social network | . 5 google.com | 0.0 | 0.000 | 0.0 | 0.0 | 0.036 | 0.0 | 0.000 | 0.0 | 0.0 | 0.000 | 0.797 | 0.000 | 0.0 | 0.000 | 0.047 | 0.000 | search engine | . 6 yahoo.com | 0.0 | 0.000 | 0.0 | 0.0 | 0.000 | 0.0 | 0.000 | 0.0 | 0.0 | 0.000 | 0.253 | 0.000 | 0.0 | 0.288 | 0.116 | 0.000 | news and media | . The output of the classifier is a list of probabilities, we have selected the 3 top probabilites with the appropriate classes for each website. The class with the highest probability prediction is the category_prediction column in the web2vec dataframe. | The model classification scores depend a lot on the categories defintion and to improve the model performance you can make the categories more specific. | . Conclusion . The project implemented in this notebook attempts to tackle the problem of encoding website urls into numeric data which is very useful in a lot of machine learning problems. Another way to approach the problem is to keep the first part of scraping websites but to apply word embeddings on the text sequence instead of a bert model.Those embeddings can be summed to vectorize the website urls and map each website to a vector. The drawback of this method is the inability of using those vectors with a classical machine learning model like random forest since generaly the pretrained word embeddings can have large dimensions at least 50 an that data can be useful in training a deep learning model like LSTMs on those vectors. .",
            "url": "https://elfaizamine.github.io/data_science_blog/website%20categorization/bart/zero-shot%20learning/web2vec/text%20encoding/2022/01/05/Websites-Categorization.html",
            "relUrl": "/website%20categorization/bart/zero-shot%20learning/web2vec/text%20encoding/2022/01/05/Websites-Categorization.html",
            "date": " • Jan 5, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Research: Can you train a Neural Network using Genetic Algorithm?",
            "content": "Context . The goal of this notebook is to asses the use of genetic algorithm in training neural network and updating the model weights and compare it with the classical way of using backpropagation algorithm. . Here are some information about the project: . Model : feed forward neural network | Dataset : Images of cats and non-cats | The backpropagation algorithm is developed from scratch and inspired by the deeplearning.ai course. | . To run the notebook, you must use additional files from this repository . Load Librairies and Data set . import matplotlib.pyplot as plt from load_data import load_dataset import backprop_functions as bp from PIL import Image import seaborn as sns import numpy as np import h5py import scipy import random %matplotlib inline import genetic algorithms library from deap import base from deap import creator from deap import tools import elitism from random import randint . Data understanding . # Loading the data (cat/non-cat) train_set_x_orig, train_set_y, test_set_x_orig, test_set_y, classes = load_dataset() # Cat Example index = 2 plt.imshow(train_set_x_orig[index]) print (&quot;The image with this index is labeld y = &quot; + str(train_set_y[:, index][0]) + &quot;, it&#39;s a &#39;&quot; + classes[np.squeeze(train_set_y[:, index])].decode(&quot;utf-8&quot;) + &quot;&#39; picture.&quot;) . The image with this index is labeld y = 1, it&#39;s a &#39;cat&#39; picture. . # Non-cat Example index = 3 plt.imshow(train_set_x_orig[index]) print (&quot;The image with this index is labeld y = &quot; + str(train_set_y[:, index][0]) + &quot;, it&#39;s a &#39;&quot; + classes[np.squeeze(train_set_y[:, index])].decode(&quot;utf-8&quot;) + &quot;&#39; picture.&quot;) . The image with this index is labeld y = 0, it&#39;s a &#39;non-cat&#39; picture. . # Train and Test Data Shapes m_train = train_set_x_orig.shape[0] m_test = test_set_x_orig.shape[0] num_px = train_set_x_orig.shape[1] print (&quot;Each image have a size of: (&quot; + str(num_px) + &quot;, &quot; + str(num_px) + &quot;, 3)&quot;) print (&quot;Number of training examples is: m_train = &quot; + str(m_train)) print (&quot;Number of testing examples is: m_test = &quot; + str(m_test)) print (&quot;The Height and Width of each image is: num_px = &quot; + str(num_px)) . Each image have a size of: (64, 64, 3) Number of training examples is: m_train = 209 Number of testing examples is: m_test = 50 The Height and Width of each image is: num_px = 64 . Data Preparation . # Reshape the training and test examples to be ingested by a feed forward neural network train_set_x_flatten = train_set_x_orig.reshape(train_set_x_orig.shape[0], -1).T test_set_x_flatten = test_set_x_orig.reshape(test_set_x_orig.shape[0], -1).T # Standarize the inputs train_set_x = train_set_x_flatten/255. test_set_x = test_set_x_flatten/255. print (&quot;The new train shape input is: &quot; + str(train_set_x_flatten.shape)) print (&quot;The train shape output is: &quot; + str(train_set_y.shape)) . The new train shape input is: (12288, 209) The train shape output is: (1, 209) . Gradient Descent &amp; Backpropagation . Model Training . # model function that will compute the cost using gradient descent and do backpropagation to train model def model(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.5, print_cost = False): &quot;&quot;&quot; Builds the logistic regression model by calling the function implemented previously Arguments: X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train) Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train) X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test) Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test) num_iterations -- hyperparameter representing the number of iterations to optimize the parameters learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize() print_cost -- Set to true to print the cost every 100 iterations Returns: d -- dictionary containing information about the model. &quot;&quot;&quot; # initialize parameters with zeros w, b = bp.initialize_with_zeros(X_train.shape[0]) # Gradient descent parameters, grads, costs = bp.optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost) # Retrieve parameters w and b from dictionary &quot;parameters&quot; w = parameters[&quot;w&quot;] b = parameters[&quot;b&quot;] # Predict test/train set examples Y_prediction_test = bp.predict(w, b, X_test) Y_prediction_train = bp.predict(w, b, X_train) # Print train/test Errors print(&quot;train accuracy using gradient descent: {} %&quot;.format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100)) print(&quot;test accuracy using gradient descent: {} %&quot;.format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100)) d = {&quot;costs&quot;: costs, &quot;Y_prediction_test&quot;: Y_prediction_test, &quot;Y_prediction_train&quot; : Y_prediction_train, &quot;w&quot; : w, &quot;b&quot; : b, &quot;learning_rate&quot; : learning_rate, &quot;num_iterations&quot;: num_iterations} return d . . Results . # Results of the model # The gradient result model achieved test accuracy of: 72% result = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = 1000, learning_rate = 0.005, print_cost = True) . Cost after iteration 0: 0.693147 Cost after iteration 100: 0.584508 Cost after iteration 200: 0.466949 Cost after iteration 300: 0.376007 Cost after iteration 400: 0.331463 Cost after iteration 500: 0.303273 Cost after iteration 600: 0.279880 Cost after iteration 700: 0.260042 Cost after iteration 800: 0.242941 Cost after iteration 900: 0.228004 train accuracy: 96.65071770334929 % test accuracy: 72.0 % . . Genetic Algorithm . In this part I am going to train a one layer neural network using genetic algorithm. In other words, finding the optimal set of values &quot;Weight Vector&quot; that will minimize the cost function simply through iteration and Fitness values. . The chromosome representation of the GA will the the weight vector itself of the Neural Network. | We will start with a population of chromosome (potential solutions) of POPULATION_SIZE. | Apply a selection process &quot;Tournament selection&quot; to choose the best parents of the next generation. | Apply a crossover process &quot;SimulatedBinaryBounded&quot; within boundaries BOUND_LOW, BOUND_UP | Apply a mutation process &quot;mutPolynomialBounded&quot; within boundaries BOUND_LOW, BOUND_UP | Use elitism to pass the x best solution fom generation G to G+1 with no modification. | Repeat the process until a stopping condition is reached : here is MAX_GENERATIONS. | . Functions and Hyperparameters . # function that create one individual using random real values between low and up def randomFloat(low , up): return [random.uniform(l, u) for l, u in zip([low] * (DIMENSIONS+1), [up] * (DIMENSIONS+1))] # Fitness function:score that will let the GA knows wich solution to choose to be the parents of next generation def cross_entropy_loss(individual, X_train=train_set_x, Y_train=train_set_y) : eps = 1e-5 m = X_train.shape[1] w = np.array(individual[:-1]) b = individual[-1] A = bp.sigmoid(np.dot(w.T,X_train)+b) cost = round(-(1/m) * np.sum(Y_train*np.log(A+eps)+(1-Y_train)*np.log(1-A+eps)),3) , return cost . . # Parameters of the Genetic Algorithms DIMENSIONS = train_set_x.shape[0] # Length of chromosome thar reprensent the weights BOUND_LOW, BOUND_UP = -0.1, 0.1 # Boundaries of weight values POPULATION_SIZE = 100 # Population size P_CROSSOVER = 0.7 # probability for crossover P_MUTATION = 0.3 # probability for mutating an individual MAX_GENERATIONS = 100 # max generations to ierate HALL_OF_FAME_SIZE = 20 # Number of individuals to pass from G to G+1 without crossover nor mutation CROWDING_FACTOR = 10.0 # crowding factor for crossover and mutation random.seed(42) # Definition of algorithms to compute the GA pipeline using DEAP toolbox = base.Toolbox() creator.create(&quot;FitnessMin&quot;, base.Fitness, weights=(-1.0,)) creator.create(&quot;Individual&quot;, list, fitness=creator.FitnessMin) toolbox.register(&quot;attrFloat&quot;, randomFloat, BOUND_LOW, BOUND_UP) toolbox.register(&quot;individualCreator&quot;, tools.initIterate, creator.Individual, toolbox.attrFloat) toolbox.register(&quot;populationCreator&quot;, tools.initRepeat, list, toolbox.individualCreator) toolbox.register(&quot;evaluate&quot;, cross_entropy_loss) toolbox.register(&quot;select&quot;, tools.selTournament,tournsize = 4) toolbox.register(&quot;mate&quot;, tools.cxSimulatedBinaryBounded, low=BOUND_LOW, up=BOUND_UP, eta=CROWDING_FACTOR) toolbox.register(&quot;mutate&quot;, tools.mutPolynomialBounded, low=BOUND_LOW, up=BOUND_UP, eta=CROWDING_FACTOR, indpb=10.0/DIMENSIONS) . . Model Training . # model function that will train the model using fitness values def model(toolbox, X_train, Y_train, X_test, Y_test): &quot;&quot;&quot; Builds the logistic regression model by calling the function implemented previously but training with GA algorithm Arguments: X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train) Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train) X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test) Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test) toolbox -- GA class that saves optimisation parameters and algorithms to be used later Returns: d -- dictionary containing information about the model. &quot;&quot;&quot; population = toolbox.populationCreator(n=POPULATION_SIZE) # prepare the statistics object: stats = tools.Statistics(lambda ind: ind.fitness.values) stats.register(&quot;min&quot;, np.min) stats.register(&quot;avg&quot;, np.mean) # define the hall-of-fame object: hof = tools.HallOfFame(HALL_OF_FAME_SIZE) # perform the Genetic Algorithm flow with elitism: population, logbook = elitism.eaSimpleWithElitism(population, toolbox, cxpb=P_CROSSOVER, mutpb=P_MUTATION, ngen=MAX_GENERATIONS, stats=stats, halloffame=hof, verbose=True) # Infos on best solution found: best = hof.items[0] # extract statistics: minFitnessValues, meanFitnessValues = logbook.select(&quot;min&quot;, &quot;avg&quot;) # plot statistics: sns.set_style(&quot;whitegrid&quot;) plt.plot(minFitnessValues, color=&#39;red&#39;) plt.plot(meanFitnessValues, color=&#39;green&#39;) plt.xlabel(&#39;Generation&#39;) plt.ylabel(&#39;Min / Average Fitness&#39;) plt.title(&#39;Min and Average fitness over Generations&#39;) plt.show() # Retrieve parameters w and b from dictionary &quot;parameters&quot; w = np.array(best[:-1]) b = best[-1] # Predict test/train Y_prediction_test = bp.predict(w, b, X_test) Y_prediction_train = bp.predict(w, b, X_train) print(&quot;train accuracy: {} %&quot;.format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100)) print(&quot;test accuracy: {} %&quot;.format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100)) d = {&quot;costs&quot;: best.fitness.values[0], &quot;Y_prediction_test&quot;: Y_prediction_test, &quot;Y_prediction_train&quot; : Y_prediction_train, &quot;w&quot; : w, &quot;b&quot; : b} return d . . Results . Hyperparameter variation : Boundaries of weight values . # BOUND_LOW, BOUND_UP = -0.5, 0.5 (the weight values) # Test accuracy : 58% Result = model(toolbox, train_set_x, train_set_y, test_set_x, test_set_y) . gen nevals min avg 0 100 2.241 3.94754 1 58 2.211 2.99987 2 65 1.789 2.83352 3 55 1.789 2.60072 4 55 1.602 2.42983 5 62 1.602 2.33102 6 52 1.602 2.37952 7 57 1.539 2.04481 8 51 1.539 2.04603 9 58 1.539 2.12531 10 61 1.403 1.9389 11 60 1.403 1.95289 12 51 1.403 1.83036 13 61 1.384 1.86647 14 58 1.376 1.56289 15 55 1.349 1.4422 16 54 1.344 1.39843 17 54 1.344 1.37828 18 54 1.336 1.36181 19 58 1.329 1.3518 20 47 1.326 1.34517 21 57 1.326 1.34066 22 56 1.326 1.33443 23 58 1.316 1.3306 24 56 1.313 1.32599 25 50 1.311 1.32098 26 58 1.308 1.31675 27 57 1.307 1.31374 28 54 1.303 1.31182 29 53 1.297 1.30938 30 50 1.296 1.30667 31 52 1.294 1.30319 32 54 1.291 1.29944 33 56 1.284 1.29604 34 56 1.279 1.29231 35 51 1.279 1.28843 36 56 1.273 1.28546 37 61 1.273 1.28224 38 52 1.27 1.27894 39 54 1.258 1.27544 40 57 1.258 1.27193 41 58 1.256 1.26717 42 56 1.253 1.26292 43 59 1.252 1.25916 44 58 1.247 1.25704 45 56 1.247 1.25423 46 60 1.241 1.25164 47 55 1.241 1.24831 48 56 1.236 1.24513 49 56 1.235 1.24251 50 56 1.231 1.2396 51 57 1.227 1.23698 52 51 1.225 1.23418 53 54 1.225 1.23115 54 61 1.221 1.22855 55 54 1.22 1.2256 56 55 1.217 1.22441 57 55 1.213 1.2211 58 54 1.213 1.21911 59 54 1.209 1.21672 60 58 1.207 1.21456 61 55 1.205 1.21213 62 51 1.203 1.2101 63 49 1.198 1.20756 64 53 1.196 1.20511 65 58 1.195 1.20238 66 55 1.193 1.1994 67 59 1.189 1.19673 68 51 1.187 1.19454 69 53 1.182 1.19158 70 45 1.18 1.18902 71 56 1.178 1.18627 72 47 1.178 1.18339 73 52 1.173 1.18156 74 53 1.17 1.17923 75 60 1.166 1.17717 76 63 1.166 1.17456 77 47 1.158 1.17134 78 56 1.158 1.16806 79 58 1.157 1.16517 80 52 1.155 1.16163 81 51 1.151 1.15874 82 53 1.147 1.15671 83 60 1.144 1.15379 84 52 1.142 1.15061 85 59 1.141 1.14821 86 54 1.135 1.14556 87 59 1.135 1.14202 88 62 1.131 1.1388 89 52 1.127 1.13583 90 49 1.127 1.13403 91 55 1.126 1.13146 92 63 1.124 1.12952 93 63 1.119 1.12797 94 51 1.119 1.12547 95 60 1.116 1.12368 96 54 1.115 1.12183 97 54 1.112 1.11971 98 61 1.104 1.11722 99 61 1.104 1.11505 100 60 1.104 1.11239 . train accuracy: 70.33492822966508 % test accuracy: 58.0 % . . # BOUND_LOW, BOUND_UP = -0.1, 0.1 (Smaller search space) # Test accuracy : 62% Result = model(toolbox, train_set_x, train_set_y, test_set_x, test_set_y) . gen nevals min avg 0 100 0.771 1.46543 1 58 0.771 0.9899 2 53 0.722 0.90938 3 61 0.706 0.87544 4 51 0.706 0.88109 5 56 0.675 0.89101 6 54 0.643 0.82273 7 58 0.643 0.78794 8 56 0.643 0.76938 9 59 0.643 0.72664 10 62 0.612 0.72597 11 59 0.612 0.67766 12 47 0.593 0.65913 13 63 0.593 0.68578 14 62 0.593 0.64169 15 60 0.589 0.62839 16 59 0.577 0.61321 17 61 0.568 0.60937 18 51 0.568 0.59778 19 54 0.563 0.5909 20 61 0.563 0.58798 21 53 0.553 0.58162 22 55 0.553 0.57064 23 60 0.539 0.57775 24 56 0.539 0.56588 25 53 0.539 0.56069 26 45 0.539 0.55624 27 53 0.532 0.55899 28 56 0.532 0.54795 29 58 0.527 0.54504 30 60 0.526 0.539 31 57 0.526 0.53629 32 59 0.524 0.53528 33 56 0.524 0.53071 34 58 0.524 0.52796 35 57 0.524 0.52631 36 57 0.52 0.526 37 61 0.52 0.52581 38 58 0.52 0.5235 39 46 0.518 0.52188 40 56 0.516 0.5209 41 54 0.515 0.51965 42 56 0.515 0.51881 43 56 0.514 0.51746 44 54 0.513 0.51606 45 53 0.512 0.51525 46 64 0.511 0.51456 47 52 0.51 0.51361 48 58 0.51 0.51294 49 49 0.51 0.51222 50 57 0.509 0.51209 51 56 0.508 0.51082 52 58 0.508 0.50997 53 56 0.507 0.50913 54 54 0.506 0.5083 55 49 0.505 0.50771 56 53 0.505 0.50717 57 56 0.504 0.50641 58 66 0.504 0.50562 59 53 0.503 0.50487 60 52 0.502 0.50441 61 54 0.502 0.50372 62 50 0.502 0.50303 63 50 0.501 0.50261 64 48 0.501 0.50212 65 56 0.501 0.50208 66 60 0.501 0.50178 67 51 0.5 0.50135 68 51 0.5 0.50107 69 58 0.5 0.50072 70 46 0.499 0.50028 71 46 0.499 0.50017 72 47 0.499 0.49997 73 55 0.498 0.49981 74 56 0.498 0.4994 75 59 0.498 0.49908 76 55 0.498 0.49886 77 59 0.498 0.49852 78 55 0.497 0.49819 79 52 0.497 0.49808 80 61 0.497 0.49808 81 53 0.496 0.49778 82 57 0.496 0.49734 83 54 0.496 0.49703 84 47 0.495 0.49668 85 59 0.495 0.49625 86 51 0.495 0.49597 87 65 0.495 0.4957 88 55 0.494 0.49523 89 50 0.494 0.49508 90 53 0.494 0.49494 91 59 0.493 0.4946 92 52 0.493 0.49413 93 50 0.493 0.49403 94 52 0.493 0.49396 95 55 0.493 0.49381 96 51 0.492 0.4934 97 55 0.492 0.49318 98 48 0.492 0.49308 99 51 0.492 0.49297 100 56 0.492 0.49277 . train accuracy: 76.55502392344498 % test accuracy: 66.0 % . . . Note: Hyperparameter Analysis. . The bounding range improves the GA choice of values since it is less space to search. Accuracy from 58% to 62%. | From the two graphs we see that the min and average fitness converge early on approximately the Geneartion 40.We conclude that the diversity inside the population disapears quickly. | To keep the population diversified: increse probability of mutation of individual (P_MUTATION) &amp; minimise number of best solutions passed (HALL_OF_FAME_SIZE) | . Hyperparameter variation : mutation and crossover . # P_CROSSOVER = 0.6, P_MUTATION = 0.4, HALL_OF_FAME_SIZE = 10 Result = model(toolbox, train_set_x, train_set_y, test_set_x, test_set_y) . gen nevals min avg 0 100 0.771 1.46543 1 64 0.763 0.97667 2 68 0.714 0.97071 3 58 0.714 0.87276 4 64 0.691 0.80997 5 62 0.689 0.81111 6 71 0.669 0.81154 7 67 0.669 0.77923 8 66 0.627 0.7431 9 73 0.622 0.7175 10 64 0.61 0.74455 11 65 0.61 0.6892 12 68 0.602 0.66283 13 69 0.602 0.62883 14 68 0.596 0.6255 15 78 0.596 0.64238 16 66 0.583 0.62473 17 67 0.581 0.61421 18 72 0.562 0.62368 19 73 0.557 0.60044 20 73 0.557 0.59567 21 70 0.554 0.5994 22 64 0.545 0.58325 23 68 0.537 0.57174 24 69 0.536 0.57145 25 68 0.533 0.55576 26 66 0.527 0.54757 27 57 0.523 0.53688 28 66 0.523 0.53582 29 71 0.517 0.5304 30 67 0.517 0.5276 31 72 0.51 0.52447 32 70 0.51 0.52048 33 60 0.51 0.5167 34 70 0.507 0.51428 35 74 0.505 0.512 36 68 0.505 0.50968 37 75 0.503 0.50794 38 68 0.498 0.50746 39 75 0.497 0.50523 40 60 0.497 0.50297 41 65 0.496 0.50175 42 74 0.494 0.49945 43 67 0.494 0.49782 44 74 0.492 0.49641 45 66 0.492 0.49504 46 74 0.49 0.49405 47 69 0.489 0.49271 48 81 0.489 0.49185 49 71 0.488 0.49071 50 63 0.486 0.48982 51 67 0.486 0.48892 52 68 0.486 0.48856 53 65 0.485 0.4875 54 59 0.485 0.48676 55 67 0.484 0.48603 56 69 0.484 0.48534 57 64 0.483 0.48468 58 68 0.482 0.48447 59 64 0.482 0.4839 60 73 0.482 0.48339 61 60 0.481 0.48279 62 78 0.481 0.48233 63 80 0.48 0.48189 64 60 0.48 0.4813 65 70 0.48 0.48104 66 68 0.479 0.4807 67 68 0.479 0.48022 68 69 0.478 0.47995 69 70 0.478 0.47925 70 63 0.477 0.47872 71 64 0.477 0.47814 72 74 0.477 0.47809 73 67 0.477 0.47778 74 74 0.476 0.47737 75 56 0.476 0.47691 76 64 0.475 0.47651 77 65 0.475 0.47616 78 71 0.474 0.47588 79 60 0.474 0.47527 80 74 0.474 0.47495 81 73 0.473 0.47452 82 69 0.473 0.47416 83 59 0.472 0.47368 84 72 0.471 0.47304 85 69 0.471 0.47246 86 66 0.471 0.47208 87 76 0.471 0.47159 88 69 0.47 0.47123 89 69 0.47 0.47102 90 75 0.469 0.47074 91 67 0.469 0.47039 92 66 0.468 0.47013 93 66 0.468 0.46981 94 66 0.468 0.46926 95 75 0.468 0.46864 96 67 0.467 0.46822 97 69 0.466 0.46788 98 69 0.466 0.46748 99 63 0.466 0.46692 100 70 0.465 0.46654 . train accuracy: 81.33971291866028 % test accuracy: 54.0 % . . # increse ipnb to 1000 : probability of mutation inside each individual(weights mutations), P_CROSSOVER = 0.8, P_MUTATION = 0.2 Result = model(toolbox, train_set_x, train_set_y, test_set_x, test_set_y) . gen nevals min avg 0 100 0.771 1.46543 1 68 0.771 1.02503 2 66 0.73 0.95984 3 74 0.73 0.98152 4 67 0.726 0.9101 5 64 0.71 0.85006 6 65 0.679 0.8373 7 72 0.661 0.8359 8 68 0.661 0.79814 9 71 0.643 0.76927 10 68 0.621 0.78521 11 69 0.607 0.73339 12 70 0.586 0.73494 13 68 0.586 0.70797 14 73 0.572 0.67897 15 72 0.565 0.68997 16 71 0.565 0.6223 17 65 0.552 0.61038 18 66 0.552 0.60557 19 65 0.547 0.57607 20 66 0.542 0.57464 21 63 0.534 0.56496 22 65 0.53 0.5597 23 69 0.528 0.55846 24 67 0.518 0.54838 25 66 0.518 0.54234 26 69 0.511 0.54322 27 60 0.503 0.53506 28 65 0.498 0.52928 29 66 0.498 0.52709 30 73 0.498 0.52292 31 62 0.491 0.51743 32 71 0.491 0.51499 33 64 0.489 0.50427 34 74 0.484 0.50503 35 70 0.478 0.50212 36 65 0.472 0.49462 37 75 0.471 0.49488 38 69 0.471 0.48824 39 63 0.466 0.48483 40 69 0.463 0.48414 41 65 0.463 0.47885 42 67 0.462 0.47676 43 53 0.458 0.47309 44 62 0.457 0.47043 45 67 0.457 0.46631 46 74 0.456 0.46522 47 67 0.455 0.46522 48 63 0.447 0.46133 49 73 0.447 0.46205 50 72 0.447 0.45972 51 70 0.447 0.45905 52 73 0.445 0.45338 53 63 0.443 0.4546 54 68 0.442 0.45095 55 67 0.442 0.44906 56 71 0.44 0.44934 57 69 0.44 0.44705 58 66 0.439 0.44705 59 57 0.439 0.44548 60 72 0.438 0.44668 61 62 0.438 0.4442 62 65 0.434 0.44153 63 76 0.433 0.44232 64 66 0.431 0.43954 65 71 0.431 0.43894 66 70 0.431 0.43852 67 68 0.43 0.43904 68 68 0.429 0.43526 69 62 0.428 0.43527 70 55 0.427 0.43241 71 60 0.427 0.4315 72 71 0.423 0.43211 73 71 0.423 0.43158 74 65 0.421 0.42913 75 68 0.421 0.42934 76 73 0.42 0.42684 77 70 0.42 0.4257 78 66 0.418 0.42551 79 60 0.418 0.42458 80 66 0.417 0.42349 81 68 0.417 0.42213 82 68 0.416 0.42187 83 67 0.415 0.42067 84 67 0.414 0.4196 85 72 0.414 0.41916 86 66 0.411 0.4183 87 68 0.411 0.41718 88 68 0.41 0.41588 89 65 0.41 0.41751 90 68 0.409 0.41466 91 70 0.407 0.41332 92 65 0.407 0.41373 93 74 0.407 0.41294 94 73 0.405 0.4136 95 71 0.405 0.41075 96 70 0.404 0.41006 97 64 0.403 0.40766 98 71 0.403 0.40712 99 66 0.401 0.40755 100 61 0.399 0.40606 . train accuracy: 84.68899521531101 % test accuracy: 60.0 % . . . Note: Hyperparameter Analysis. . increasing the ipnb keeps the population diversified more since the mean and min not exactly the same through generations. | The slope of the graph after Geneartion 40 is largely negative due to the mutation aspect. | . Conclusion . GA is difficult to train , due to the high number of Hyperparameters and dimensions of search space, and with no guidance from the outside world but the value of the cost function. . | GA is suited for problems that are too complex to find a mathematical representation or where the loss function contains too many local minimums. In this problem, the backpropagation Algorithm is expected to outperform the GA, since the cost function is differentiable, and the weight update is performed intelligently with the calculation of gradients. . | A more suited approach is to combine both algorithms to benefit from the global optimisation aspect of the GA and prevent Backprop Algorithms in getting stucked in local minimums. . | .",
            "url": "https://elfaizamine.github.io/data_science_blog/genetic%20algorithm/backpropagation/gradient%20descent/neural%20network/2020/05/20/train-ann-using-genetic-algorithm.html",
            "relUrl": "/genetic%20algorithm/backpropagation/gradient%20descent/neural%20network/2020/05/20/train-ann-using-genetic-algorithm.html",
            "date": " • May 20, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://elfaizamine.github.io/data_science_blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://elfaizamine.github.io/data_science_blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}