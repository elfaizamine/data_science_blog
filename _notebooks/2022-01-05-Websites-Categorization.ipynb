{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41a352f4",
   "metadata": {},
   "source": [
    "# \"Natural Language Processing: Website Categorization\"\n",
    "> Classify websites urls into categories using pretrained Bart Model\n",
    "\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- author: Amine EL FAIZ\n",
    "- categories: [Website Categorization, Bart, Zero-shot Learning, web2vec, text encoding]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a2697e",
   "metadata": {},
   "source": [
    "# Context\n",
    "\n",
    "The goal of this project is websites categorization which refers to the process of classifying websites into various categories based on their content and purpose. For example, amazon website can be classified into e-commerce category.\n",
    "\n",
    "Website categorization either manually or by using machine learning algorithms can be useful in many areas:\n",
    "- Content filtering: blocking certain websites dependent on their categories from access by certain users.\n",
    "- Contextual Marketing: Allows businesses to display ads on pages that are similar or relevant to the product or service they offer.\n",
    "- Brand protection: Looking for copycat websites that are similar to yours but harm your brand.\n",
    "- Text Data Encoding: Transform websites text data into vectors that have meaning like a website embedding vector and websites with similar purpose have close vectors.\n",
    "\n",
    "I have come across this project when trying to encode websites text data into vectors to train a machine learning model on those vectors. One of the ways is to classify each website into a set of categories either by probability or classes.\n",
    "The advantage of using this method besides the vectorization is:\n",
    "- We can sum up multiple vectors and the result would be meaningful for example in case of a user who visited a series of websites and we want to encode this sequence of visits.\n",
    "- If a machine learning model is trained on these categories we can interpret the results by doing feature importance.\n",
    "\n",
    "To obtain this objective the process of transformation is as follows:\n",
    "- Scrap from website URLs text data.\n",
    "- Define the categories that will define the content of each website.\n",
    "- Predict the probability that the scraped text belongs to each of the defined categories using pretrained Bart Model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b15821e",
   "metadata": {},
   "source": [
    "# Load Librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9740e8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer \n",
    "import cloudscraper\n",
    "from bs4 import BeautifulSoup\n",
    "from googletrans import Translator\n",
    "from tldextract import extract\n",
    "import transformers\n",
    "from transformers import pipeline\n",
    "from transformers import AutoModel\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "from collections import Counter\n",
    "from googletrans import Translator\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from tldextract import extract\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd70b1cf",
   "metadata": {},
   "source": [
    "# Websites Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7c3b842",
   "metadata": {},
   "outputs": [],
   "source": [
    "#collapse-show\n",
    "# Function that will scrapp a desired website\n",
    "def scrap_website(scraper, headers, website_name, text_size):\n",
    "    allthecontent = ''\n",
    "    try:\n",
    "        r = scraper.get(website_name, headers=headers)\n",
    "        soup = BeautifulSoup(r.text, 'html.parser')\n",
    "        title = soup.find('title').text\n",
    "        description = soup.find('meta', attrs={'name': 'description'})\n",
    "        if \"content\" in str(description):\n",
    "            description = description.get(\"content\")\n",
    "        else:\n",
    "            description = \"\"\n",
    "        h1 = soup.find_all('h1')\n",
    "        h1_all = \"\"\n",
    "        for x in range(len(h1)):\n",
    "            if x == len(h1) - 1:\n",
    "                h1_all = h1_all + h1[x].text\n",
    "            else:\n",
    "                h1_all = h1_all + h1[x].text + \". \"\n",
    "        paragraphs_all = \"\"\n",
    "        paragraphs = soup.find_all('p')\n",
    "        for x in range(len(paragraphs)):\n",
    "            if x == len(paragraphs) - 1:\n",
    "                paragraphs_all = paragraphs_all + paragraphs[x].text\n",
    "            else:\n",
    "                paragraphs_all = paragraphs_all + paragraphs[x].text + \". \"\n",
    "        h2 = soup.find_all('h2')\n",
    "        h2_all = \"\"\n",
    "        for x in range(len(h2)):\n",
    "            if x == len(h2) - 1:\n",
    "                h2_all = h2_all + h2[x].text\n",
    "            else:\n",
    "                h2_all = h2_all + h2[x].text + \". \"\n",
    "        h3 = soup.find_all('h3')\n",
    "        h3_all = \"\"\n",
    "        for x in range(len(h3)):\n",
    "            if x == len(h3) - 1:\n",
    "                h3_all = h3_all + h3[x].text\n",
    "            else:\n",
    "                h3_all = h3_all + h3[x].text + \". \"\n",
    "        allthecontent = str(title) + \" \" + str(description) + \" \" + str(h1_all) + \" \" + str(h2_all) + \" \" + str(\n",
    "            h3_all) + \" \" + str(paragraphs_all)\n",
    "        allthecontent = str(allthecontent)[0:text_size]\n",
    "    except Exception as e:\n",
    "        pass\n",
    "    return allthecontent\n",
    "\n",
    "def translate_sentence(translator, sentence, text_size):\n",
    "    translation = translator.translate(sentence)\n",
    "    translation = str(translation)[0:text_size]\n",
    "    return translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0e7453bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Scraped Text after cleaning and english translation of: https://www.amazon.com is: Amazon.com. Spend less. Smile more. Free shipping on millions of items. Get the best of Shopping and Entertainment with Prime. Enjoy low prices and great deals on the largest selection of everyday essentials and other products, including fashion, home, beauty, electronics, Alexa Devices, sporting goods, toys, automotive, pets, baby, books, video games, musical instruments, office supplies, and more.  Sign in for the best experience. Explore Departments  \n",
      "The Scraped Text after cleaning and english translation of: https://www.youtube.com is: YouTube Share your videos with friends, family, and the world    \n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Scarp websites and translate words for non english websites\n",
    "scraper = cloudscraper.create_scraper() \n",
    "headers = {'user-agent': 'Mozilla/5.0 (Linux; Android 6.0.1; Nexus 5X Build/MMB29P) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/85.0.4183.121 Mobile Safari/537.36 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)'}\n",
    "translator = Translator()\n",
    "\n",
    "# Example of Amazon\n",
    "website_name = 'https://www.amazon.com'\n",
    "website_content = scrap_website(scraper, headers, website_name, text_size = 700)\n",
    "website_content_en = translate_sentence(translator, website_content, text_size= 700)\n",
    "website_content_en = ' '.join(re.findall('[a-z]+', website_content.lower()))\n",
    "print('The Scraped Text after cleaning and english translation of:', website_name, 'is:',website_content)\n",
    "\n",
    "# Example of youtube\n",
    "website_name = 'https://www.youtube.com'\n",
    "website_content = scrap_website(scraper, headers, website_name, text_size = 700)\n",
    "website_content_en = translate_sentence(translator, website_content, text_size = 700)\n",
    "website_content_en = ' '.join(re.findall('[a-z]+', website_content.lower()))\n",
    "print('The Scraped Text after cleaning and english translation of:', website_name, 'is:',website_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa3b235",
   "metadata": {},
   "source": [
    "# Input and Output Definition\n",
    "\n",
    "We will define the categories that the bart model will choose from and the websites we want to classify:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "90a39a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# websites to classify\n",
    "websites = ['amazon.com','instagram.com','wikipedia.org','netflix.com','facebook.com','google.com','yahoo.com']\n",
    "# Categories from bart model to choose from and to labelize each website base on front page content\n",
    "candidate_labels = ['health','e-commerce','advertising', 'job','computer','education','entertainment',\\\n",
    "                            'home and family','industry','Information Technology','search engine',\\\n",
    "                            'social network','science','news and media','read', 'buisness']\n",
    "# Tabular DataFrame containing vectors\n",
    "web2vec = pd.DataFrame(websites, columns=['website_url'])\n",
    "web2vec[candidate_labels] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57dd53f7",
   "metadata": {},
   "source": [
    "# Categorization using BART classifier\n",
    "For further learning about the facebook Bart Classification Model, refer to this [link](https://huggingface.co/facebook/bart-large-mnli)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f1fe239e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/bart-large-mnli were not used when initializing BartModel: ['classification_head.out_proj.weight', 'classification_head.dense.bias', 'classification_head.out_proj.bias', 'classification_head.dense.weight']\n",
      "- This IS expected if you are initializing BartModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "#collapse-output\n",
    "# Initialise Bart model that will transform website to useful encoding\n",
    "# Needs Internet Connection Must be downloaded before \n",
    "model_name = \"facebook/bart-large-mnli\" \n",
    "# Download pytorch model\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "# Load Bert Zero shot classification model\n",
    "classifier = pipeline(\"zero-shot-classification\",\n",
    "                      model=\"facebook/bart-large-mnli\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "be01f319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sequence': 'amazon com spend less smile more free shipping on millions of items get the best of shopping and entertainment with prime enjoy low prices and great deals on the largest selection of everyday essentials and other products including fashion home beauty electronics alexa devices sporting goods toys automotive pets baby books video games musical instruments office supplies and more sign in for the best experience explore departments', 'labels': ['e-commerce', 'entertainment', 'read', 'buisness', 'home and family', 'industry', 'search engine', 'advertising', 'job', 'computer', 'health', 'social network', 'science', 'Information Technology', 'education', 'news and media'], 'scores': [0.601190447807312, 0.1486208140850067, 0.09820207953453064, 0.037607591599226, 0.033550821244716644, 0.02008289285004139, 0.0157319363206625, 0.014724561013281345, 0.008524124510586262, 0.006259399000555277, 0.0036483018193393946, 0.0029156305827200413, 0.002454340225085616, 0.002257096115499735, 0.0021261561196297407, 0.0021038008853793144]}\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "# Categorization Prediction of amazon website\n",
    "website_name = 'https://www.amazon.com'\n",
    "website_content = scrap_website(scraper, headers, website_name, text_size = 700)\n",
    "website_content_en = translate_sentence(translator, website_content, text_size= 700)\n",
    "website_content_en = ' '.join(re.findall('[a-z]+', website_content.lower()))\n",
    "output = classifier(website_content_en, candidate_labels)\n",
    "print(output)\n",
    "# The top probability class prediction is e-commerce which is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "25724b69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Website url amazon.com is being classified\n",
      "Website url instagram.com is being classified\n",
      "Website url wikipedia.org is being classified\n",
      "Website url netflix.com is being classified\n",
      "Website url facebook.com is being classified\n",
      "Website url google.com is being classified\n",
      "Website url yahoo.com is being classified\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# the encode_websites function group all steps defined earlier to makje a classification\n",
    "# Apply the pipeline on websites defined earlier\n",
    "webList = web2vec.website_url.to_list()\n",
    "for i in range(web2vec.shape[0]):\n",
    "    print('Website url', webList[i], 'is being classified')\n",
    "    try:\n",
    "        output = encode_websites(classifier, scraper, headers,translator, website_name = webList[i], text_size = 500)\n",
    "        if output != 0:\n",
    "            web2vec.loc[i,output['labels'][:3]] = output['scores'][:3]\n",
    "        else:\n",
    "            pass\n",
    "    except Exception as e:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "699d0b51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>website_url</th>\n",
       "      <th>health</th>\n",
       "      <th>e-commerce</th>\n",
       "      <th>advertising</th>\n",
       "      <th>job</th>\n",
       "      <th>computer</th>\n",
       "      <th>education</th>\n",
       "      <th>entertainment</th>\n",
       "      <th>home and family</th>\n",
       "      <th>industry</th>\n",
       "      <th>Information Technology</th>\n",
       "      <th>search engine</th>\n",
       "      <th>social network</th>\n",
       "      <th>science</th>\n",
       "      <th>news and media</th>\n",
       "      <th>read</th>\n",
       "      <th>buisness</th>\n",
       "      <th>Category_Prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>amazon.com</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.601</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.149</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.000</td>\n",
       "      <td>e-commerce</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>instagram.com</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.820</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>social network</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>wikipedia.org</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.141</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.304</td>\n",
       "      <td>0.102</td>\n",
       "      <td>read</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>netflix.com</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.296</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.169</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.000</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>facebook.com</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.767</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.000</td>\n",
       "      <td>social network</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>google.com</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.797</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.000</td>\n",
       "      <td>search engine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>yahoo.com</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.253</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.288</td>\n",
       "      <td>0.116</td>\n",
       "      <td>0.000</td>\n",
       "      <td>news and media</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     website_url  health  e-commerce  advertising  job  computer  education  \\\n",
       "0     amazon.com     0.0       0.601          0.0  0.0     0.000        0.0   \n",
       "1  instagram.com     0.0       0.000          0.0  0.0     0.000        0.0   \n",
       "2  wikipedia.org     0.0       0.000          0.0  0.0     0.141        0.0   \n",
       "3    netflix.com     0.0       0.000          0.0  0.0     0.000        0.0   \n",
       "4   facebook.com     0.0       0.000          0.0  0.0     0.000        0.0   \n",
       "5     google.com     0.0       0.000          0.0  0.0     0.036        0.0   \n",
       "6      yahoo.com     0.0       0.000          0.0  0.0     0.000        0.0   \n",
       "\n",
       "   entertainment  home and family  industry  Information Technology  \\\n",
       "0          0.149              0.0       0.0                   0.000   \n",
       "1          0.061              0.0       0.0                   0.038   \n",
       "2          0.000              0.0       0.0                   0.000   \n",
       "3          0.296              0.0       0.0                   0.169   \n",
       "4          0.000              0.0       0.0                   0.070   \n",
       "5          0.000              0.0       0.0                   0.000   \n",
       "6          0.000              0.0       0.0                   0.000   \n",
       "\n",
       "   search engine  social network  science  news and media   read  buisness  \\\n",
       "0          0.000           0.000      0.0           0.000  0.098     0.000   \n",
       "1          0.000           0.820      0.0           0.000  0.000     0.000   \n",
       "2          0.000           0.000      0.0           0.000  0.304     0.102   \n",
       "3          0.000           0.000      0.0           0.000  0.180     0.000   \n",
       "4          0.000           0.767      0.0           0.000  0.071     0.000   \n",
       "5          0.797           0.000      0.0           0.000  0.047     0.000   \n",
       "6          0.253           0.000      0.0           0.288  0.116     0.000   \n",
       "\n",
       "  Category_Prediction  \n",
       "0          e-commerce  \n",
       "1      social network  \n",
       "2                read  \n",
       "3       entertainment  \n",
       "4      social network  \n",
       "5       search engine  \n",
       "6      news and media  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "web2vec.fillna(0, inplace=True)\n",
    "web2vec[candidate_labels] = round(web2vec[candidate_labels], 3)\n",
    "web2vec['Category_Prediction'] = web2vec[candidate_labels].idxmax(axis=1)\n",
    "web2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e607c8a5",
   "metadata": {},
   "source": [
    "- The output of the classifier is a list of probabilities, we have selected the 3 top probabilities with the appropriate classes for each website.\n",
    "The class with the highest probability prediction is the category_prediction column in the web2vec dataframe.\n",
    "- The model classification scores depend a lot on the categories definition and to improve the model performance you can make the categories more specific."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256c4501",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "The project implemented in this notebook attempts to tackle the problem of encoding website URLs into numeric data which is very useful in a lot of machine learning problems.\n",
    "Another way to approach this problem is to keep the first part of websites scraping but to apply word embeddings on the text sequence instead of a Bert model. These embeddings can be summed to vectorize the website URLs and map each website to a vector. The drawback of this method is the inability to use these vectors with a classical machine learning model like random forest since generally the pre-trained word embeddings can have large dimensions of at least 50 but it can be useful in training deep learning models like LSTMs.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test1",
   "language": "python",
   "name": "test1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
